% Useful variables
\newcommand{\DocMainTitle}{UncertaintyQuantification.jl}
\newcommand{\DocVersion}{}
\newcommand{\DocAuthors}{Jasper Behrensdorf and Ander Gray}
\newcommand{\JuliaVersion}{1.11.4}

% ---- Insert preamble
\input{preamble.tex}


\part{Home}




\chapter{UncertaintyQuantification.jl}



\label{10326402411837190628}{}


A Julia package for uncertainty quantification.



\section{Authors}



\label{3633061387637902549}{}




\section{Features}



\label{7269310191258299096}{}


Current functionality includes:



\begin{itemize}
\item Simulation-based reliability analysis

\begin{itemize}
\item Monte Carlo simulation


\item Quasi Monte Carlo simulation (Sobol, Halton)


\item (Advanced) Line Sampling


\item Subset Simulation

\end{itemize}

\item Sensitivity analysis

\begin{itemize}
\item Gradients


\item Sobol indices

\end{itemize}

\item Metamodeling

\begin{itemize}
\item Polyharmonic splines


\item Response Surface


\item Polynomial Chaos Expansion

\end{itemize}

\item Bayesian Updating


\item Third-party solvers

\begin{itemize}
\item Connect to any solver by injecting random samples into source files


\item HPC interfacing with slurm

\end{itemize}

\item Stochastic Dynamics

\begin{itemize}
\item Power Spectral Density Estimation


\item Stochastic Process Generation

\end{itemize}
\end{itemize}


{\rule{\textwidth}{1pt}}


\section{Installation}



\label{16848947351613073531}{}


To install the latest release through the Julia package manager run:




\begin{minted}{julia}
julia> ]add UncertaintyQuantification
julia> using UncertaintyQuantification
\end{minted}



or install the latest development version with:




\begin{minted}{julia}
julia> ]add UncertaintyQuantification#master
julia> using UncertaintyQuantification
\end{minted}



{\rule{\textwidth}{1pt}}


\subsection{Related packages}



\label{5607455135763110590}{}


\begin{itemize}
\item \href{https://github.com/cossan-working-group/OpenCossan}{OpenCossan}: Matlab-based toolbox for uncertainty quantification and management

\end{itemize}


\part{Manual}


\chapter{Introduction}


\section{Introduction}



\label{16879307210612674453}{}


\subsection{Uncertainty}



\label{10991333852424530659}{}


The definition of \emph{uncertainty} follows from the absence of \emph{certainty} describing a state of absolute knowledge where everything there is to know about a process is known [\hyperlinkref{17677777724040662142}{1}]. This however is a theoretical and unachievable state in which deterministic models would be sufficient for the analysis of engineering systems. In reality, there is always a gap between certainty and the current state of knowledge resulting in \emph{uncertainty}.



\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{assets/uncertainty.svg}}
\caption{Characterization of uncertainties}
\end{figure}
 \emph{Characterization of reducible and irreducible uncertainties. Adapted from [\hyperlinkref{5841225584812469631}{2}]}.



Although a topic of ongoing debate and revision [\hyperlinkref{5841225584812469631}{2}], it is largely accepted that uncertainty can be broadly classified into two types, \emph{aleatory} and \emph{epistemic} uncertainty [\hyperlinkref{15087536451490649448}{3}]. The first type, \emph{aleatory} uncertainties, are also called \emph{irreducible} uncertainties or \emph{variability} and describe the inherent randomness of a process. This could, for example, be variability in material properties, degradation of components, or varying external forces such as wind loads or earthquakes. Some researchers debate the existence of aleatory uncertainty under the assumption that if a process was fully understood it would no longer be random. Epistemic uncertainty is the uncertainty resulting from a lack of knowledge or vagueness and is also called \emph{reducible} uncertainty, as it can be reduced through the collection of additional data and information. If both types of uncertainties occur together this is sometimes called \emph{hybrid} or \emph{mixed} uncertainty, and can be modelled using imprecise probability.



We follow Bi et al. [\hyperlinkref{3640249115689488589}{4}] in dividing uncertainties into four categories:     - Category I: Constant parameters without any associated uncertainty,     - Category II: Parameters only subject to epistemic uncertainty represented as intervals,     - Category III: Variables with only aleatory uncertainties, fully described by probability distributions,     - Category IV: Variables subject to both aleatory and epistemic uncertainty represented using imprecise probabilities, for example using probability boxes.



Next follows a brief introduction to the modelling of precise (category III) and imprecise probabilities (category IV).



A continuous random variable \(X\) is uniquely defined by its cumulative density function (CDF) \(F: \mathbb{R} \rightarrow [0,1]\). By definition, it returns the probability that the random variable will take a value less than or equal to \(x\)



\begin{equation*}
\begin{split}    F_X(x) = P_X(X \leq x).\end{split}\end{equation*}


A CDF is a right continuous and monotonically non-decreasing function which satisfies



\begin{equation*}
\begin{split}    \lim_{x\rightarrow -\infty} F_X(x) = 0\end{split}\end{equation*}


and



\begin{equation*}
\begin{split}    \lim_{x\rightarrow \infty}  F_X(x) = 1.\end{split}\end{equation*}


The probability density function (PDF) of a random variable can be obtained as the derivative of the CDF



\begin{equation*}
\begin{split}    f_X(x) = \frac{dF_X(x)}{dx},\end{split}\end{equation*}


if it exists. Conversely, the CDF can be defined as the integral of the PDF as



\begin{equation*}
\begin{split}    F_X(x) = \int_{-\infty}^x f_X(\lambda) d\lambda.\end{split}\end{equation*}


Using the PDF and CDF random variables subject to aleatory uncertainty can be described using well established probability theory.



Ferson et al. [\hyperlinkref{13630934291971494037}{5}] introduced the notion of a \emph{probability box} (p-box) for the representing variables witg both epistemic and aleatory uncertainty. Consider two CDFs \(\underline{F}\) and \(\overline{F}\) with \(\underline{F}(x) \leq \overline{F}(x)\) for all \(x \in \mathbb{R}\). Then, \([\underline{F}(x), \overline{F}(x)]\) is the set of CDFs \(F\) such that \(\underline{F}(x) \leq F(x) \leq \overline{F}(x)\). This set is called the p-box for an imprecisely known random variable \(X\), where \(\underline{F}(x)\) is the lower bound for the probability that \(X\) is smaller than or equal to \(x\), and \(\overline{F}(x)\) is the upper bound of this probability.



The simplest way to construct a p-box is using a known parametric family (normal, exponential, ...) with intervals for their parameters (mean, variance, ...), and from this we can form the set \([\underline{F}(x), \overline{F}(x)]\). This is known as parametric p-box, only containing distributions following the specified distribution family. If no family information is available, but \([\underline{F}(x), \overline{F}(x)]\) are known, called a \emph{distribution-free} p-box, where every possible CDF between the bounds is a valid random variable.



Special algorithms must be used to propagate the epistemic uncertainty through models. As a result, the analysis also returns upper and lower bounds. This propagation of the epistemic uncertainty comes with a significant increase in computational demand, requiring specialised algorithms or perhaps surrogate modelling.



In \emph{UncertaintyQuantification.jl} the four categories of uncertainties are described using the following objects:



\begin{itemize}
\item Category I: \hyperlinkref{16741877178295688592}{\texttt{Parameter}}


\item Category II: \hyperlinkref{16181946445135277992}{\texttt{Interval}}


\item Category III: \hyperlinkref{4023900003980544990}{\texttt{RandomVariable}}


\item Category IV: \hyperlinkref{460210473987211826}{\texttt{ProbabilityBox}}.

\end{itemize}


\chapter{Getting Started}


\section{Getting Started}



\label{12615906682062737037}{}


Here we introduce the basic building blocks of \emph{UncertaintyQuantification}. This includes the inputs such as \texttt{Parameter} or \texttt{RandomVariable} which will feed into any \texttt{Model} for a variety of different analyses. We will also present more advanced concepts including how to model dependencies between the inputs through copulas.



\subsection{Inputs}



\label{509063468412031875}{}


\subsubsection{Parameters}



\label{12886555095302773432}{}


A \texttt{Parameter} is defined as a constant scalar value. In addition to value the constructor also requires a \texttt{Symbol} by which it can later be identified in the \texttt{Model}. A \texttt{Symbol} is a Julia object which is often used as a name or label. \texttt{Symbol}s are defined using the \texttt{:} prefix. Parameters represent constant deterministic values. As an example we define a \texttt{Parameter} representing the gravity of Earth.




\begin{minted}{julia}
g = Parameter(9.81, :g)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Parameter(9.81, :g)
\end{minted}



Parameters are very handy when constants show up in the \texttt{Model} in multiple spaces. Instead of updating every instance in the \texttt{Model}, we can conveniently update the value by changing a single line.



\subsubsection{Random Variables}



\label{4893463912507967731}{}


A \texttt{RandomVariable} is essentially a wrapper around any \texttt{UnivariateDistribution} defined in the \emph{Distributions.jl} package [\hyperlinkref{4623761477557390374}{6}]. Similarly to the \texttt{Parameter}, the second argument to the constructor is a \texttt{Symbol} acting as a unique identifier. For example, a standard gaussian random variable is defined by passing \texttt{Normal()} and \texttt{:x} as arguments.




\begin{minted}{julia}
x = RandomVariable(Normal(), :x)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
RandomVariable(Normal{Float64}(μ=0.0, σ=1.0), :x)
\end{minted}



A list of all possible distributions can be generated by executing \texttt{subtypes(UnivariateDistribution)} in the Julia REPL (read-eval-print loop). Note that, \emph{Distributions} is re-exported from \emph{UncertaintyQuantification} and no separate \texttt{using} statement is necessary. In addition, the most important methods of the \texttt{UnivariateDistribution} including \texttt{pdf}, \texttt{cdf}, and \texttt{quantile}, are also defined for the \texttt{RandomVariable}.



Random samples can be drawn from a \texttt{RandomVariable} by calling the \texttt{sample} method passing the random variable and the desired number of samples.




\begin{minted}{julia}
samples = sample(x, 100) # sample(x, MonteCarlo(100))
\end{minted}



The \texttt{sample} method returns a \texttt{DataFrame} with the samples in a single column. When sampling from a \texttt{Vector} of random variables these individual columns are automatically merged into one unified \texttt{DataFrame}. By default, this will use standard Monte Carlo simulation to obtain the samples. Alternatively, any of the quasi-Monte Carlo methods can be used instead.




\begin{minted}{julia}
samples = sample(x, SobolSampling(100))
samples = sample(x, LatinHypercubeSampling(100))
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
┌ Warning: n must be a power of 2, automatically increased to 128
└ @ UncertaintyQuantification A:\src\github.com\FriesischScott\UncertaintyQuantification.jl\src\simulations\montecarlo.jl:16
\end{minted}



Many of the advanced simulations, e.g. line sampling or subset simulation require mappings to (and from) the standard normal space, and these are exposed through the \texttt{to\_standard\_normal\_space!} and \texttt{to\_physical\_space!} methods respectively. These operate on a \texttt{DataFrame} and as such can be applied to samples directly. The transformation is done in-place, i.e. no new \texttt{DataFrame} is returned. As such, in the following example, the samples end up exactly as they were in the beginning.




\begin{minted}{julia}
to_standard_normal_space!(x, samples)
to_physical_space!(x, samples)
\end{minted}



\subsection{Dependencies}



\label{1748328383054528076}{}


\emph{UncertaintyQuantification} supports modelling of dependencies through copulas. By using copulas, the modelling of the dependence structure is separated from the modelling of the univariate marginal distributions. The basis for copulas is given by Sklar{\textquotesingle}s theorem [\hyperlinkref{15494272573324799614}{7}]. It states that any multivariate distribution \(H\) in dimensions \(d \geq 2\) can be separated into its marginal distributions \(F_i\) and a copula function \(C\).



\begin{equation*}
\begin{split}H(x_1,\ldots,x_2) = C(F_1(x_1),\ldots,F_d(x_d))\end{split}\end{equation*}


For a thorough discussion of copulas, see [\hyperlinkref{4026133216555554139}{8}].



In line with Sklar{\textquotesingle}s theorem we build the joint distribution of two dependent random variables by separately defining the marginal distributions.




\begin{minted}{julia}
x = RandomVariable(Normal(), :x)
y = RandomVariable(Uniform(), :y)
marginals = [x, y]
\end{minted}



Next, we define the copula to model the dependence. \emph{UncertaintyQuantification} supports Gaussian copulas for multivariate \(d \geq 2\) dependence. Here, we define a Gaussian copula by passing the correlation matrix and then build the \texttt{JointDistribution} from the copula and the marginals.




\begin{minted}{julia}
cop = GaussianCopula([1 0.8; 0.8 1])
joint = JointDistribution(marginals, cop)
\end{minted}



\subsection{Models}



\label{3607757037139922397}{}


In this section we present the models included in \emph{UncertaintyQuantification}. A model, in its most basic form, is a relationship between a set of input variables \(x \in \mathbb{R}^{n_x}\) and an output \(y \in \mathbb{R}\). Currently, most models are assumed to return single-valued outputs. However, as seen later, the \texttt{ExternalModel} is capable of extracting an arbitrary number of outputs from a single run of an external solver.



\subsubsection{Model}



\label{12360836855591423082}{}


A \texttt{Model} is essentially a native Julia function operating on the previously defined inputs. Building a \texttt{Model} requires two things: a \texttt{Function}, which is internally passed a \texttt{DataFrame} containing the samples and must return a \texttt{Vector} containing the model response for each sample, and a \texttt{Symbol} which is the identifier used to add the model output into the \texttt{DataFrame}.



Suppose we wanted to define a \texttt{Model} which computes the distance from the origin of two variables \(x\) and \(y\) as \(z\). We first define the function and then pass it to the \texttt{Model}.




\begin{minted}{julia}
function z(df::DataFrame)
  return @. sqrt(df.x^2 + df.y^2)
end
m = Model(z, :z)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Model(Main.z, :z)
\end{minted}



An alternative for a simple model such as this, is to directly pass an anonymous function to the \texttt{Model}.




\begin{minted}{julia}
m = Model(df -> sqrt.(df.x.^2 .+ df.y.^2), :z)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Model(Main.var"#1#2"(), :z)
\end{minted}



After defining it, a \texttt{Model} can be evaluated on a set of samples by calling the \texttt{evaluate!} method. This will add the model outcome to the \texttt{DataFrame}. Alternatively, the reponse can be obtained as a vector, by calling the \texttt{Model} as a function.




\begin{minted}{julia}
samples = sample([x, y], MonteCarlo(1000))
evaluate!(m, samples) # add to the DataFrame
output = m(samples) # return a Vector
\end{minted}



However, most of the time manual evaluation of the \texttt{Model} will not be necessary as it is done internally by whichever analysis is performed.



\subsubsection{ExternalModel}



\label{11984978887985385641}{}


The \texttt{ExternalModel} provides interaction with almost any third-party solver. The only requirement is, that the solver uses text-based input and output files in which the values sampled from the random variables can be injected for each individual run. The output quantities are then extracted from the files generated by the solver using one (or more) \texttt{Extractor}(s). This way, the simulation techniques included in this module, can be applied to advanced models in finite element software such as \emph{OpenSees} or \emph{Abaqus}.



The first step in building the \texttt{ExternalModel} is to define the folder where the source files can be found as well as the working directory. Here, we assume that the source file for a simple supported beam model is located in a subdirectory of our current working directory. Similarly, the working directory for the solver is defined. In addition, we define the exact files where values need to be injected, and any extra files required. No values will be injected into the files specified as extra. In this example, no extra files are needed, so the variable is defined as an empty \texttt{String} vector.




\begin{minted}{julia}
sourcedir = joinpath(pwd(), "demo/models")
sourcefiles = ["supported-beam.tcl"]
extrafiles = String[]
workdir = joinpath(pwd(), "supported-beam")
\end{minted}






\begin{minted}{text}
nDMaterial ElasticIsotropic 1 {{{ :E }}} 0.25 {{{ :rho }}}
\end{minted}



This identifies where to inject the values, but not in which format. For this reason, we define a \texttt{Dict\{Symbol, String\}} which maps the identifiers of the inputs to a Python-style format string. In order to inject our values in scientific notation with eight digits, we use the format string \texttt{{\textquotedbl}.8e{\textquotedbl}}. For any not explicitly defined \texttt{Symbol} we can include \texttt{:*} as a fallback.




\begin{minted}{julia}
formats = Dict(:E => ".8e",:rho => ".8e", :* => ".12e")
\end{minted}



After formatting and injecting the values into the source file, it would look similar to this.




\begin{minted}{text}
nDMaterial ElasticIsotropic 1 9.99813819e+02 0.25 3.03176259e+00
\end{minted}



Now that the values are injected into the source files, the next step is to extract the desired output quantities. This is done using an \texttt{Extractor}. The \texttt{Extractor} is designed similarly to the \texttt{Model} in that it takes a \texttt{Function} and a \texttt{Symbol} as its parameters. However, where a \texttt{DataFrame} is passed to the \texttt{Model}, the working directory for the currently evaluated sample is passed to the function of the \texttt{Extractor}. The user defined function must then extract the required values from the file and return them. Here, we make use of the \emph{DelimitedFiles} module to extract the maximum absolute displacement from the output file that \emph{OpenSees} generated.




\begin{minted}{julia}
disp = Extractor(base -> begin
  file = joinpath(base, "displacement.out")
  data = readdlm(file, ' ')

  return maximum(abs.(data[:, 2]))
end, :disp)
\end{minted}



An arbitrary number of \texttt{Extractor} functions can be defined in order to extract multiple output values from the solver.



The final step before building the model is to define the solver. The solver requires the path to the binary, and the input file. Optional command line arguments can be passed to the \texttt{Solver} through the \texttt{args} keyword. If the solver binary is not on the system path, the full path to the executable must be defined. Finally, the \texttt{ExternalModel} is assembled.




\begin{minted}{julia}
opensees = Solver(
  "OpenSees",
  "supported-beam.tcl";
  args = ""
)

ext = ExternalModel(
  sourcedir, sourcefiles, disp, opensees; formats=numberformats, workdir=workdir, extras=extrafiles
)
\end{minted}



A full example of how to run a reliability analysis of a model defined in \emph{OpenSees} can be found in the demo files.



\chapter{Kernel Density Estimation}


\section{Kernel Density Estimation}



\label{6883224486742304299}{}


Kernel density estimation (KDE) is a non-parametric method to estimate the probability density function of a random variable through \emph{kernel smoothing} [\hyperlinkref{3820492032986589588}{9}].



The kernel density estimate \(\hat{f}_h\) of a univariate density \texttt{f} based on a random sample \(X_1,\ldots,X_n\) is defined as



\begin{equation*}
\begin{split}\hat{f}_h(x) = n^{-1} \sum_{i=1}^n h^{-1} K \left\{\frac{x-X_i}{h}\right\},\end{split}\end{equation*}


where \(h\) is the so called \emph{bandwidth} and \(K\) is the kernel function. The kernel function is assumed to be a symmetric probability density and is set to be a Gaussian density in \emph{UncertaintyQuantification.jl}. The bandwidth \(h\) also called the \emph{smoothing parameter} has a strong effect on the resulting density estimate. There are various different methods to select an optimal bandwidth. Here we have decided to apply the method developed by Sheather \& Jones [\hyperlinkref{15023702097247238757}{10}] for its excellent performance and straightforward implementation.



The kernel density estimation is exposed through the \hyperlinkref{18426214934190206718}{\texttt{EmpiricalDistribution}}. Since the bandwidth is automatically selected only a vector containing the data must be passed to the constructor.




\begin{minted}{julia}
 d = EmpiricalDistribution(x)
\end{minted}



Internally, we perform the kernel density estimation to obtain the \emph{PDF} of the distribution. From this PDF we estimate the support of the distribution through numerical root finding. The \emph{CDF} and the quantile function (inverse CDF) are interpolated from the numerical integral of the PDF. The number of points used for this interpolation (defaults to \(10^4\)) can be passed to the constructor as an optional second parameter. As a \texttt{ContinousUnivariateDistribution} the \hyperlinkref{18426214934190206718}{\texttt{EmpiricalDistribution}} can be applied the same as any of the native distributions from \emph{Distributions.jl}.



\subsection{Example}



\label{12204569949998619624}{}


As an example we consider synthetic data generated from a bimodal distribution and fit the empirical distribution.




\begin{minted}{julia}
x = [rand(Normal(5), 500)..., rand(Normal(10), 500)...]
ed = EmpiricalDistribution(x)
\end{minted}



Next, we plot the normalized histogram of the data and the resulting PDF.





\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{manual/kernel-density.svg}}
\caption{Kernel Density Plot}
\end{figure}




\chapter{Reliability Analysis}


\section{Reliability Analysis}



\label{5425193432845275192}{}


In the context of structural engineering, engineering design, and risk assessment, the term reliability is used to describe the ability of system to perform its intended function under varying conditions over time.



There, the state of a system is identified by its \emph{performance function} (also sometimes refered to as limit state function) \(g(\boldsymbol{x})\) such that:



\begin{equation*}
\begin{split}g(\boldsymbol{x}) =
\begin{cases}
    > 0 & \text{safe\ domain}\\
    \leq 0 & \text{failure \ domain}\\
\end{cases}.\end{split}\end{equation*}


Then the \emph{probability of failure} is defined as the likelihood of the system being in the failed state, given as



\begin{equation*}
\begin{split}p_f = \int_{g(\boldsymbol{x}) \leq 0} f_{\boldsymbol{X}}(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}.\end{split}\end{equation*}


Here, \(f_{\boldsymbol{X}}(\boldsymbol{x})\) denotes the joint probability density function (PDF) of the input \(\boldsymbol{X}\).



\subsection{Definition of the Input, Model and Performance}



\label{5805987019592958816}{}


The first step of the implementation of a reliability analysis in \texttt{UncertaintyQuantification.jl} is the definition of the probabilistic input and the model which is shown exemplarily.



Here we use a modification of the first example presented in [\hyperlinkref{5290438035122008794}{11}] which uses a quadratic performance function and a probabilistic input containing of two standard normal random variables \(\boldsymbol{X} = [X_1, X_2]\) with \(X_i \sim \mathcal{N}(0,1)\). The model is given as



\begin{equation*}
\begin{split}y(\boldsymbol{X}) = 0.1(X_1 - X_2)^2 - \frac{1}{\sqrt{2}} (X_1 + X_2).\end{split}\end{equation*}


Then, the performance function is defined as



\begin{equation*}
\begin{split}g(\boldsymbol{X}) = y(\boldsymbol{X}) + 4.\end{split}\end{equation*}


The probabilistic input is implemented as




\begin{minted}{julia}
x = RandomVariable.(Normal(), [:x1, :x2])
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
2-element Vector{RandomVariable}:
 RandomVariable(Normal{Float64}(μ=0.0, σ=1.0), :x1)
 RandomVariable(Normal{Float64}(μ=0.0, σ=1.0), :x2)
\end{minted}



Next we define the model for the response \(y(\boldsymbol{X})\) as




\begin{minted}{julia}
y = Model(df -> 0.1*(df.x1 - df.x2).^2 - 1/sqrt(2) * (df.x1 + df.x2), :y)
\end{minted}



where the first input is the function \(y\) (which must accept a \texttt{DataFrame}) and the second argument is the \texttt{Symbol} for the output variable. With the help of the model, we can define the performance function \(g\) which again takes a \texttt{DataFrame} as an input:




\begin{minted}{julia}
g(df) = df.y .+ 4
\end{minted}



\subsection{Approximation Methods}



\label{13997252733228183761}{}


\subsubsection{First Order Reliability Method}



\label{9085550815434865802}{}


The First Order Reliability Method (FORM) [\hyperlinkref{9935717079447158324}{12}] estimates the failure probability by finding a linear approximation of the performance function at the so-called \emph{design point} \(\boldsymbol{U}^*\). The design point represents the point on the surface of the performance function \(g(\boldsymbol{X}) = 0\) that is closest to the origin in the standard normal space.



That distance from the design point to the origin is referred to as the \emph{reliability index} given as \(\beta^* = ||\boldsymbol{U}^*||\). Due to the transformation to the standard normal space, the probability of failure is simply given as



\begin{equation*}
\begin{split}\hat{p}_{f, \mathrm{FORM}} = \Phi(-\beta^*)\end{split}\end{equation*}


where \(\Phi\) denotes the standard normal CDF.



In addition to the \(\beta^*\), the location of the design point is specified by the \emph{important direction} defined as:



\begin{equation*}
\begin{split}\boldsymbol{\alpha}^* = \frac{\boldsymbol{U}^*}{||\boldsymbol{U}^*||}.\end{split}\end{equation*}


In \texttt{UncertaintyQuantification.jl} a FORM analysis can be performed calling \texttt{probability\_of\_failure(model, performance, input, simulation)} where \texttt{FORM()} is passed as the simulation method:




\begin{minted}{julia}
pf_form, β, dp = probability_of_failure(y, g, x, FORM())

println("Probability of failure: $pf_form")
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Probability of failure: 3.1671241833194436e-5
\end{minted}



\subsection{Simulation Methods}



\label{8677960879144690738}{}


\subsubsection{Monte Carlo Simulation}



\label{5834098574690153248}{}


Monte Carlo Simulation (MCS) offers an approximation of the failure probability using stochastic simulation.



It utilizes an indicator function of the failure domain



\begin{equation*}
\begin{split}\mathbb{I}[g(\boldsymbol{x})] =
\begin{cases}
    0 & \text{when} \ g(\boldsymbol{x}) > 0\\
    1 & \text{when} \ g(\boldsymbol{x}) \leq 0\\
\end{cases}.\end{split}\end{equation*}


This allows for the failure probability to be interpreted as the expected value of the indicator function



\begin{equation*}
\begin{split}p_f = \int_{\boldsymbol{X}} \mathbb{I}[g(\boldsymbol{x})]\ f_{\boldsymbol{X}}(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} = \mathbb{E}\big[\mathbb{I}[g(\boldsymbol{x})]\big].\end{split}\end{equation*}


The Monte Carlo estimate of the failure probability is given as



\begin{equation*}
\begin{split}p_f \approx \hat{p}_f = \frac{1}{N} \sum_{i=1}^N \mathbb{I}[g(\boldsymbol{x}_i)]\end{split}\end{equation*}


where \(\{\boldsymbol{x}_i\}_{i=1}^N\) represents a set of \(N\) samples drawn from the input PDF \(f_{\boldsymbol{X}}(\boldsymbol{x})\). The variance of the estimator is given as



\begin{equation*}
\begin{split}\operatorname{Var}[\hat{p}_f] = \frac{\hat{p}_f (1-\hat{p}_f)}{N}.\end{split}\end{equation*}


In \texttt{UncertaintyQuantification.jl} we can perform a Monte Carlo Simulation by defining the analysis as \texttt{MonteCarlo(n)}  where \texttt{n} is the number of samples:




\begin{minted}{julia}
mc = MonteCarlo(10^7)
\end{minted}



Then the reliability analysis is performed by calling \texttt{probability\_of\_failure(model, performance, input, simulation)}.




\begin{minted}{julia}
pf_mc, std_mc, samples = probability_of_failure(y, g, x, mc)

println("Probability of failure: $pf_mc")
println("Coefficient of variation: $(std_mc/pf_mc)")
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Probability of failure: 1.82e-5
Coefficient of variation: 0.07412425712616279
\end{minted}



\subsubsection{Importance Sampling}



\label{16432856939035910975}{}


Based on the standard MCS method, a class of advanced method exist that have to goal to accelerate the estimation of the failure probability by requiring fewer model calls.



Importance Sampling [\hyperlinkref{7384485740282643180}{13}] introduces a second density that is \emph{biased} in a way that it generates more samples in the failure domain. Typically such a density is constructed around the design point obtained in a preceding FORM analysis.



In order to perform a reliability analysis using Importance Sampling, we again have to specify the number of samples and then can \texttt{probability\_of\_failure()}.




\begin{minted}{julia}
is = ImportanceSampling(1000)
pf_is, std_is, samples = probability_of_failure(y, g, x, is)

println("Probability of failure: $pf_is")
println("Coefficient of variation: $(std_is/pf_is)")
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Probability of failure: 2.0801590194268694e-5
Coefficient of variation: 0.04739109089805725
\end{minted}



\subsubsection{Radial Based Importance Sampling}



\label{2106574880283869235}{}


Radial based importance sampling (RBIS) [\hyperlinkref{8183389152391755847}{14}] increases the efficiency of the Monte Carlo simulation by sampling in standard normal space and excluding a β-sphere where no failures occur from the sampling domain. Here, \texttt{β} is the reliability index obtained from a preliminary analysis like FORM. The probability of failure is then estimated as



\begin{equation*}
\begin{split}p_f \approx \hat{p}_f = (1- \chi^2_k(\beta^2) \frac{1}{N} \sum_{i=1}^N \mathbb{I}[g(\boldsymbol{x}_i)],\end{split}\end{equation*}


where \(\chi^2_k\) is the CDF of the Chi-squared distribution with \(k\) degrees of freedom and \(k\) is the number of random variables.



If no \texttt{β} or \texttt{β=0.0} is passed to the \hyperlinkref{4916152994621835685}{\texttt{RadialBasedImportanceSampling}} constructor, a FORM analysis will automatically be performed.




\begin{minted}{julia}
rbis = RadialBasedImportanceSampling(1000)
pf_rbis, std_rbis, samples = probability_of_failure(y, g, x, rbis)

println("Probability of failure: $pf_rbis")
println("Coefficient of variation: $(std_rbis/pf_rbis)")
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Probability of failure: 1.9456832418391024e-5
Coefficient of variation: 0.12744167022738218
\end{minted}



A scatter plot clearly shows the exclusion of the β-sphere. \begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{manual/rbis-samples.svg}}
\caption{RBIS samples}
\end{figure}




\subsubsection{Line Sampling}



\label{12507279593722163479}{}


Another advanced Monte Carlo method for reliability analysis is Line Sampling [\hyperlinkref{9920150670139184364}{15}]. Its main idea is to use parallel lines for sampling rather than points.



Therefore first the problem is transformed into the standard normal space to make use of the invariance of rotation. The important direction \(\boldsymbol{\alpha}\) is determined, e.g., using FORM or the gradient at the origin. Then, samples are generated and projected onto the hyperplane orthogonal to \(\boldsymbol{\alpha}\). From each point on the hyperplane, a line is drawn parallel to \(\boldsymbol{\alpha}\) and its intersection with the performance function is determined using root finding based on a spline interpolation scheme, giving the set of distances \(\{\beta^{(i)}\}_{i=1}^N\) from the hyperplane to the intersection with the performance function. Due to working in the standard normal space, the \emph{failure probability along each line} is given as



\begin{equation*}
\begin{split}p_{f, \mathrm{line}}^{(i)} = \Phi(-\beta^{(i)})\end{split}\end{equation*}


Finally, the probability of failure is obtained as the mean of the failure probabilities along the lines



\begin{equation*}
\begin{split}\hat{p}_{f,\mathrm{LS}} = \frac{1}{N} \sum_{i=1}^N p_{f, \mathrm{line}}^{(i)}.\end{split}\end{equation*}


The variance of \(\hat{p}_{f,\mathrm{LS}}\) is given by the variance of the line failure probabilities:



\begin{equation*}
\begin{split}\operatorname{Var}[\hat{p}_{f,\mathrm{LS}}] = \frac{1}{N(N-1)} \sum_{i=1}^N \Big(p_{f, \mathrm{line}}^{(i)} - \hat{p}_{f,\mathrm{LS}}\Big)^2.\end{split}\end{equation*}


Similar to standard MCS, we have to pass \(N\) to the Line Sampling method. However, here we pass the number of lines. Optionally, we can pass a vector of the points along each line that are used to evaluate the performance function and a predetermined direction \(\boldsymbol{\alpha}\):




\begin{minted}{julia}
ls = LineSampling(100, collect(0.5:0.5:8.0))
pf_ls, std_ls, samples = probability_of_failure([y], g, x, ls)

println("Probability of failure: $pf_ls")
println("Coefficient of variation: $(std_ls/pf_ls)")
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Probability of failure: 1.759892913118257e-5
Coefficient of variation: 0.05972209531902244
\end{minted}



\subsubsection{Advanced Line Sampling}



\label{17490737439385466936}{}


Advanced Line Sampling [\hyperlinkref{12068672950609541734}{16}] is a further enhancement of the standard line sampling methods due to two main features:



\begin{itemize}
\item[1. ] The important direction \(\boldsymbol{\alpha}\) is adapted once a more probable point is found


\item[2. ] The lines are processed sorted by proximity of the points on the hyperplane.

\end{itemize}


Especially the second point enables the use of an iterative root finder using Newton{\textquotesingle}s method.



The definition of the \texttt{AdvancedLineSampling} simulation method is similar to that of regular Line Sampling. The number of lines has to be given to the constructor and we can optionally give the number of points along the line which is only used to find the starting point of the iterative root search.




\begin{minted}{julia}
als = AdvancedLineSampling(100, collect(0.5:0.5:10))
pf_als, std_als, samples = probability_of_failure([y], g, x, als)

println("Probability of failure: $pf_als")
println("Coefficient of variation: $(std_als/pf_als)")
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Probability of failure: 1.9239198664210635e-5
Coefficient of variation: 0.054014678924743736
\end{minted}



For \texttt{AdvancedLineSampling}, we can also define the (initial) direction and options of the iterative root finding, i.e., the \texttt{tolerance}, \texttt{stepsize} of the gradient and \texttt{maxiterations}.



\begin{tcolorbox}[toptitle=-1mm,bottomtitle=1mm,colback=admonition-note!50!white,colframe=admonition-note,title=\textbf{Parallelism}]
We note that Advanced Line Sampling is a serial algorithm, although much fewer samples (order of magnitude) are required. If a large amount of parallel compute is available, standard Line Sampling may be more attractive, which is {\textquotedbl}embarrassingly{\textquotedbl} parallel like Monte Carlo.

\end{tcolorbox}


\subsection{Subset Simulation}



\label{336527219531960381}{}


Subset simulation [\hyperlinkref{15932307010839829307}{17}] is an advanced simulation technique for the estimation of small failure probabilities. This approach involves decomposing the problem into a sequence of conditional probabilities that are estimated using Markov Chain Monte Carlo.



We create the \hyperlinkref{4173351192588427739}{\texttt{SubSetSimulation}} object and compute the probability of failure using a standard Gaussian proposal PDF. The value for the target probability of failure at each intermediate level is set to \(0.1\) which is generally accepted as the optimal value.




\begin{minted}{julia}
subset = SubSetSimulation(1000, 0.1, 10, Normal())
pf_sus, std_sus, samples = probability_of_failure(y, g, x, subset)

println("Probability of failure: $pf_sus")
println("Coefficient of variation: $(std_sus/pf_sus)")
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Probability of failure: 1.4892e-5
Coefficient of variation: 0.34462771589639685
\end{minted}



Alternatively, instead of using the standard Subset simulation algorithm (which internally uses Markov Chain Monte Carlo), we can use \hyperlinkref{18397467037730927218}{\texttt{SubSetInfinity}} to compute the probability of failure, see [\hyperlinkref{345953961655055560}{18}]. Here we use a standard deviation of \(0.5\) to create the proposal samples for the next level.




\begin{minted}{julia}
subset = SubSetInfinity(1000, 0.1, 10, 0.5)
pf_sus, std_sus, samples = probability_of_failure(y, g, x, subset)

println("Probability of failure: $pf_sus")
println("Coefficient of variation: $(std_sus/pf_sus)")
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Probability of failure: 1.4200000000000003e-5
Coefficient of variation: 0.3167927200155807
\end{minted}



\chapter{Metamodelling}


\section{Metamodels}



\label{13382004127081062345}{}


\subsection{Design Of Experiments}



\label{5141534247305030192}{}


Design Of Experiments (DOE) offers various designs that can be used for creating a model of a given system. The core idea is to evaluate significant points of the system in order to obtain a sufficient model while keeping the effort to achieve this relatively low. Depending on the parameters, their individual importance and interconnections, different designs may be adequate.



The ones implemented here are \texttt{TwoLevelFactorial}, \texttt{FullFactorial}, \texttt{FractionalFactorial}, \texttt{CentralComposite}, \texttt{BoxBehnken} and \texttt{PlackettBurman}.



\subsection{Response Surface}



\label{11236368157692582757}{}


A Response Surface is a simple polynomial surrogate model. It can be trained by providing it with evaluated points of a function or any of the aforementioned experimental designs.



\chapter{Simulations}


\section{Simulations}



\label{782416527003373519}{}


\subsection{Monte Carlo}



\label{16647002951547067847}{}


The Monte-Carlo (MC) method is a method of sampling random numbers that dates back to 1777. The name was suggested by Nick Metropolis when MC was used while working on the Manhattan Project. It is used in Random Number Generators which generally produce pseudo random numbers.



\subsection{Quasi Monte Carlo}



\label{12969375726647060471}{}


Quasi Monte Carlo (QMC), is a method of producing samples similar to those generated via Monte Carlo (MC). The difference being that QMC samples are generated deterministically in a way to ensure they are evenly distributed across the sampling space, not forming clutters or voids as MC samples might. This makes QMC more efficient than MC for lots of applications since fewer samples are needed in order to produce a sufficient density of samples throughout. There are multiple ways of QMC-sampling which can be classified as either digital nets or lattices. [\hyperlinkref{3929737418388848918}{19}]



Included here are \texttt{LatticeRuleSampling} and the digital nets \texttt{SobolSampling}, \texttt{HaltonSampling}, \texttt{FaureSampling} and \texttt{LatinHaypercubeSampling}.



However, being deterministic, these QMC samples are missing the properties related to randomness that MC samples have. To gain these properties it is possible to randomize QMC samples. There are several randomization methods, useful in different cases, depending on the QMC method in use. [\hyperlinkref{3929737418388848918}{19}]



Implemented in this package are Owen-Scramble and Matousek-Scramble, two similar methods useful for Sobol and Faure Sampling aswell as Shift which can be used for Lattice Rule Sampling. There also is an algorithm for Halton Sampling, that constructs builds samples from the ground up as opposed to randomizing existing samples which is what the aforementioned methods do. [\hyperlinkref{8155427533001242046}{20}]



To sample using one of these methods, simply create an instance of the corresponding struct with the desired parameters and then call the sample function with this instance. The parameters are \texttt{n::Integer} which is the number of samples, and \texttt{randomization::Symbol} which encodes the randomization method that should be used. The different possible symbols are: \texttt{:none}, \texttt{:matousek}, \texttt{:owen}, \texttt{:shift} and \texttt{:randomizedhalton}.






\begin{minted}{julia}
    x = RandomVariable(Uniform(), :x)
    qmc = LatinHypercubeSampling(100)
    samples = sample(x, qmc)
\end{minted}



Note that not all randomization methods are possible to use for every QMC-method. Also, if no \texttt{randomization}-symbol is given, the default will be used. View the following table for details.




\begin{table}[h]
\centering
\begin{tabulary}{\linewidth}{L L C C C C C}
\toprule
QMC-method & DEFAULT & :matousek & :owen & :shift & :randomizedhalton & :none \\
\toprule
LatticeRuleSampling & :shift & ❌ & ❌ & ✅ & ❌ & ✅ \\
SobolSampling & :matousek & ✅ & ✅ & ❌ & ❌ & ✅ \\
FaureSampling & :matousek & ✅ & ✅ & ❌ & ❌ & ✅ \\
HaltonSampling & :randomizedhalton & ❌ & ❌ & ❌ & ✅ & ✅ \\
\bottomrule
\end{tabulary}

\end{table}



\begin{tcolorbox}[toptitle=-1mm,bottomtitle=1mm,colback=admonition-note!50!white,colframe=admonition-note,title=\textbf{Note}]
\texttt{LatinHypercubeSampling} is already random and thus doesn{\textquotesingle}t have the \texttt{randomization} parameter.

\end{tcolorbox}


It is of course possible to directly create the struct inside the \texttt{sample}-call, enabling a more efficient version of the example above which looks like this:




\begin{minted}{julia}
    x = RandomVariable(Uniform(), :x)
    samples = sample(x, LatinHypercubeSampling(100))
\end{minted}



\begin{tcolorbox}[toptitle=-1mm,bottomtitle=1mm,colback=admonition-note!50!white,colframe=admonition-note,title=\textbf{Note}]
When chosing \texttt{n}, bear in mind that for \texttt{SobolSampling} and \texttt{FaureSampling}, \texttt{n} must fit the base that is used for creating the respective sequence. For \texttt{SobolSampling} the base is always equal to 2 while for \texttt{FaureSampling}, it depends on the number of input-variables. If \texttt{n} is not a power of the base, it will automatically be increased to the next power.

\end{tcolorbox}



\begin{minted}{julia}
    x = RandomVariable(Uniform(), :x)
    samples = SobolSampling(100)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
SobolSampling(128, :matousek)
\end{minted}



To emphasize the importance of randomization, look at the correlations that might occur using unrandomized qmc and how they are fixed by randomizing.



This is the 7th dimension plotted against the 8th in Faure Sampling, unrandomized vs. randomized via Owen Scramble:







\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{manual/faure-sequence.svg}}
\caption{}
\end{figure}




\chapter{Bayesian Updating}


\section{Bayesian Updating}



\label{16081037215275166927}{}


Bayesian updating is a method of statistical inference where Bayes{\textquotesingle} theorem is used to update the probability distributions of model parameters based on prior beliefs and available data.



\subsection{Bayes{\textquotesingle} Theorem}



\label{537349271097409493}{}


Bayes{\textquotesingle} theorem is defined as



\begin{equation*}
\begin{split}P(\theta|Y) = \frac{P(Y|\theta)P(\theta)}{P(Y)},\end{split}\end{equation*}


where \(P(\theta)\) is the prior distribution, describing prior belief on \(\theta\). \(P(Y|\theta)\) is the likelihood function evaluating how the data Y supports our belief. This is a function of \(\theta\) not \(Y\). \(P(\theta|Y)\) is called the posterior distribution and expresses an updated belief under data \(Y\). The term \(P(Y)\), often called the marginal likelihood, or the evidence. It can be calculated as the integral of the likelihood multiplied by the prior distribution over the sample space of \(\theta\)



\begin{equation*}
\begin{split}P(Y) = \int{}P(Y|\theta)P(\theta), d\theta{}.\end{split}\end{equation*}


This term serves as a normalizing constant for the posterior probability. However, as it can be difficult or even impractical to calculate it is often disregarded. Instead, only the product of likelihood and prior is used, as it is proportional to the posterior probability



\begin{equation*}
\begin{split}P(\theta|Y) \propto P(Y|\theta)P(\theta).\end{split}\end{equation*}


Based on this relationship, the posterior probability can be approximated without calculation of \(P(Y)\) using a variety of sampling methods. Classic approaches such as rejection sampling can be inefficient, especially for multivariate cases due to high rejection rates. Instead, Metropolis et al., proposed the use of Markov chains to increase efficiency [\hyperlinkref{7743418060059618696}{21}].



\subsection{Markov Chain Monte Carlo}



\label{16362866232034031076}{}


Markov chains are sequences of variables, where each variable is dependent on the last. In a discrete space \(\Omega\) the series of random variables \(\{X_1,X_2,\ldots,X_t\}\) is called a Marko chain if



\begin{equation*}
\begin{split}p(X_t=x_t|X_{t-1}=x_{t-1},\ldots,X_1=x_1) = p(X_t=x_t|X_{t-1}=x_{t-1}) .\end{split}\end{equation*}


A Markov chain is called ergodic or irreducible when it is possible to reach each state from every other state with a positive probability. Markov chains that are ergodic and time-homogeneous, i.e. the probability between states doesn{\textquotesingle}t depend on time, and have a unique stationary distribution such that



\begin{equation*}
\begin{split}\pi(y) = \sum_{x\in\Omega}P(y|x)\pi(x).\end{split}\end{equation*}


The goal of Markov chain Monte Carlo (MCMC) sampling methods is to construct a Markov chain, whose stationary distribution is equal to the posterior distribution of Bayes{\textquotesingle} theorem. This will result in samples generated from the Markov chain being equivalent to random samples of the desired distribution. The very first MCMC algorithm is the Metropolis-Hastings (MH) Algorithm.



\subsubsection{Metropolis Hastings}



\label{17725367994195865305}{}


The Metropolis-Hastings algorithm, was published in 1970 by W. K. Hastings [\hyperlinkref{1755040324625540940}{22}]. The MH algorithm is a random-walk algorithm that provides a selection criteria for choosing the next sample \((\theta_{i + 1})\) in a Markov chain. This is done through a so-called proposal distribution \(q(\theta_{i + 1}|\theta_i)\) which is well known and relatively easy to sample from. Usually, symmetric proposal distributions centred at \((\theta_i)\) are used, for example Normal and Uniform distributions. A candidate sample \(\theta^*\) is sampled from the proposal distribution and accepted with probability \(\alpha\)



\begin{equation*}
\begin{split}\alpha = \min\left[1,\frac{P(\theta^*|Y)}{P(\theta_i|Y)}\cdot{}\frac{q(\theta_i|\theta^*)}{q(\theta^*|\theta_i)}\right].\end{split}\end{equation*}


Substituting the posterior with Bayes{\textquotesingle} theorem yields



\begin{equation*}
\begin{split}\alpha = \min\left[1,\frac{P(Y|\theta^*)\cdot{}P(\theta^*)/P(Y)}{P(Y|\theta_i)\cdot{}P(\theta_i)/P(Y)}\cdot{}\frac{q(\theta_i|\theta^*)}{q(\theta^*|\theta_i)}\right].\end{split}\end{equation*}


Note, how the normalization constant \(P(Y)\) cancels out. When the proposal is symmetric \(q(\theta_i|\theta^*) = q(\theta^*|\theta_i)\) the acceptance probability further simplifies to



\begin{equation*}
\begin{split}\alpha = \min\left[1,\frac{P(\theta^*|Y)}{P(\theta_i|Y)}\right].\end{split}\end{equation*}


In practice, a random number \(r \sim U(0,1)\) is sampled, and the candidate sample is accepted if \(a \leq r\)



\begin{equation*}
\begin{split}\theta_{i + 1} = \theta^*     \qquad  \text{if} \quad a \leq r,\\
\theta_{i + 1} = \theta_{i}   \qquad  \text{otherwise.} \\\end{split}\end{equation*}


As an example consider a synthetic data sequence \texttt{Y} as the outcome of 100 Bernoulli trials with unknown success probability \texttt{p} (here p=0.8).




\begin{minted}{julia}
 n = 100
 Y = rand(n) .<= 0.8
\end{minted}



The likelihood function which, similar to a \texttt{Model} must accept a \texttt{DataFrame}, follows a Binomial distribution and returns the likelihood for each row in the \texttt{DataFrame} as a vector. The prior is chosen as a beta distribution with \(\alpha=\beta=1\) (uniform on \([0, 1]\)). It is often beneficial to use the log-likelihood and log-prior for numerical reasons.




\begin{minted}{julia}
    function loglikelihood(df)
            return [
                sum(logpdf.(Binomial.(n, df_i.p), sum(Y))) for df_i in eachrow(df)
            ]
        end

logprior = df -> logpdf.(Beta(1,1), df.p)
\end{minted}



\textbf{UncertaintyQuantification.jl} implements a variant of the MH algorithm known as single-component Metropolis-Hastings, where the proposal and acceptance step is performed independently for each dimension. To run the algorithm, we must first define the \texttt{SingleComponentMetropolisHastings} object which requires the \texttt{UnivariateDistribution} as a \texttt{proposal}, a \texttt{NamedTuple} for \texttt{x0} which defines the starting point of the Markov chain, the number of samples and the number of burn-in samples. The burn-in samples are used to start the chain but later discarded.




\begin{minted}{julia}
    proposal = Normal(0, 0.2)
    x0 = (;p=0.5)
    n_samples= 4000
    burnin = 500

    mh = SingleComponentMetropolisHastings(proposal, x0, n_samples, burnin)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
SingleComponentMetropolisHastings(Normal{Float64}(μ=0.0, σ=0.2), (p = 0.5,), 4000, 500, true)
\end{minted}



The final optional argument \texttt{islog=true} can be omitted when passing the log-likelihood and log-prior. When set to \texttt{false}, the algorithm will \texttt{log} for both the likelihood and prior. Finally, the algorithm is executed using the \texttt{bayesianupdating} function. This function returns the samples and the average acceptance rate.




\begin{minted}{julia}
mh_samples, α   = bayesianupdating(logprior, loglikelihood, mh)
\end{minted}



The following figure shows a histogram of the samples returned by the Metropolis-Hastings algorithm. For comparison, we also plot the analytical posterior distribution obtained using \href{https://en.wikipedia.org/wiki/Conjugate\_prior}{conjugate priors} [\hyperlinkref{16839907763403924006}{23}].





\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{manual/mh.svg}}
\caption{}
\end{figure}




As a second example we will attempt to sample from a bimodal target distribution in two dimensions. The prior is uniform over \([-2, 2]\) in each dimension and the likelihood is a mixture of two Gaussian functions centred at \([0.5, 0.5]\) and \([-0.5, -0.5]\).  The standard deviation for both Gaussians are identical and if small enough will effectively disconnect the two functions.




\begin{minted}{julia}
prior = Uniform(-2, 2)
logprior = df -> logpdf.(prior, df.x) .+ logpdf.(prior, df.y)

N1 = MvNormal([-0.5, -0.5], 0.1)

N2 = MvNormal([0.5, 0.5], 0.1)

loglikelihood =
    df -> log.([0.5 * pdf(N1, collect(x)) + 0.5 * pdf(N2, collect(x)) for x in eachrow(df)])

n = 2000
burnin = 500

x0 = (; x=0.0, y=0.0)

proposal = Normal()

mh = SingleComponentMetropolisHastings(proposal, x0, n, burnin)

mh_samples, α = bayesianupdating(logprior, loglikelihood, mh)
\end{minted}



\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{manual/mh-bimodal.svg}}
\caption{}
\end{figure}




The scatter plot clearly shows that the MH algorithm has converged to only one of the two peaks of the bimodal target (contour also plotted). In fact, this is a known weakness of the MH algorithm. However, there are a number of alternative MCMC methods that aim to solve this problem. One of these methods, known as Transitional Markov Chain Monte Carlo [\hyperlinkref{7619550157776039859}{24}], will be presented next.



\subsubsection{Transitional Markov Chain Monte Carlo}



\label{3180476064451849928}{}


The Transitional Markov Chain Monte Carlo (TMCMC) method [\hyperlinkref{7619550157776039859}{24}] is an extension of the MH algorithm where instead of directly sampling a complex posterior distribution, samples are obtained from a series of simpler \textbf{transitional} distributions. The samples are obtained from independent single-step Markov Chains. The transitional distributions are defined as



\begin{equation*}
\begin{split}    P^j \propto P(Y|\theta)^{\beta_j} \cdot P(\theta),\end{split}\end{equation*}


where \(j \in \{1, \ldots, m \}\) is the number of the transition step and \(\beta_j\) is a tempering parameter with \(\beta_1 < \cdots, \beta_m =1\). This enables a slow transition from the prior to the posterior distribution. An important part of the TMCMC algorithm is that the tempering parameter has to be selected as to ensure the transition is smooth and gradual. The algorithm{\textquotesingle}s authors suggest choosing the parameter such that a coefficient of variation of 100\% is maintained in the likelihood \(P(Y\theta_i)^{\beta_j-\beta_{j - 1}}\). At each level \(j\) the starting points for the independent Markov Chains are randomly samples (with replacement) from the current set of samples using statistical weights



\begin{equation*}
\begin{split}w(\theta_i) = \frac{P(Y|\theta_i)^{\beta_j-\beta_{j-1}}}{\sum_{i=1}^N P(Y|\theta_i)^{\beta_j-\beta_{j-1}}}.\end{split}\end{equation*}


The complete TMCMC algorithm can be summarized as



\begin{itemize}
\item[1. ] Set \(j=0\) and \(\beta_j=0\). Sample \(\theta_i \sim P(\theta)\).


\item[2. ] Set \(j = j+1\).


\item[3. ] Compute the next tempering parameter \(\beta_j\).


\item[4. ] Determine the weights \(w(\theta_i)\).


\item[5. ] Generate a single-step Markov chain for each \(\theta_i\).


\item[6. ] Repeat steps (2) to (5) until (and including) \((\beta_j=1)\).

\end{itemize}


Returning to the bimodal example, this time using the TMCMC algorithm. In order to apply a different MCMC algorithm we only need to construct a \texttt{TransitionalMarkovChainMonteCarlo} object and pass it to the \texttt{bayesianupdating} method. The definition of prior and likelihood remains the same. In difference to the \texttt{SingleComponentMetropolisHastings} the log evidence is returned instead of the acceptance rate.




\begin{minted}{julia}
tmcmc = TransitionalMarkovChainMonteCarlo(RandomVariable.(Uniform(-2,2), [:x, :y]), n, burnin)

tmcmc_samples, S = bayesianupdating(logprior, loglikelihood, tmcmc)
\end{minted}



\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{manual/tmcmc.svg}}
\caption{}
\end{figure}




The resulting scatter plot shows how TMCMC is able to sample both peaks of the bimodal target distribution. The standard implementation of TMCMC uses a multivariate Gaussian proposal distribution centred at each \(\theta_i\) with covariance matrix \(\Sigma\) estimated from the current likelihood scaled by a factor \(\beta^2\). This scaling factor defaults to \(0.2\) as suggested by the authors, but can optionally be passed to the constructor as a fourth argument. Application of different MCMC Algorithms nested in the TMCMC give rise to variants of the algorithm. For example, it is possible to use the previously introduced \texttt{SingleComponentMetropolisHastings} resulting in \texttt{SingleComponentTransitionalMarkovChainMonteCarlo}.



\begin{tcolorbox}[toptitle=-1mm,bottomtitle=1mm,colback=admonition-note!50!white,colframe=admonition-note,title=\textbf{Note}]
\texttt{SingleComponentTransitionalMarkovChainMonteCarlo} is currently not available but planned for implementation.

\end{tcolorbox}


For convenience, the prior can be automatically constructed from the random variables passed to \texttt{TransitionalMarkovChainMonteCarlo}.




\begin{minted}{julia}
tmcmc = TransitionalMarkovChainMonteCarlo(RandomVariable.(Uniform(-2,2), [:x, :y]), n, burnin)

tmcmc_samples, S = bayesianupdating(loglikelihood, tmcmc)
\end{minted}



\subsection{Maximum likelihood and maximum a posteriori estimates}



\label{15340629227974867440}{}


Instead of using sample-based methods it is also possible to calculate the local maxima of likelihood (\(P(Y|\theta)\)) or posterior (\(P(\theta|Y)\)). While this does not directly give an esimate of the full parameter distribution, it allows for the estimation of the most probable parameter values. Calculating the maximum (or maxima, depending on the use case) is referred to as maximum likelihood estimate (MLE), using the full (unnormalized) posterior is referred to as maximum a posteriori (MAP) estimate. Note that technically MLE is not a Bayesian estimate as the prior distribution is ignored. More formally,



\begin{equation*}
\begin{split}\theta_{\text{MLE}} = \underset{\theta}{\arg \max} P(Y|\theta)\end{split}\end{equation*}


\begin{equation*}
\begin{split}\theta_{\text{MAP}} = \underset{\theta}{\arg \max} P(Y|\theta) P(\theta)\end{split}\end{equation*}


Generally, the MAP estimate can be considered as regularized version of the MLE, since the prior distribution is used to constrain the estimates. Both estimates are found with optimization schemes and thus come with the usual drawbacks, including convergence to local maxima. Both methods are therefore sensitive to initial conditions. Further note that both estimates do not calculate the mean but the mode of the respective distributions, i.e. for distributions that have a high variance these estimates do not provide much information about the parmater distribution.



The implementation in \textbf{UncertaintyQuantification.jl} is analgous to the sampling methods. A \texttt{MaximumAPosterioriBayesian} or a \texttt{MaximumLikelihoodBayesian} object is created that takes the important settings as input. These are the prior, the optimization method to use and the starting points for optimization. For convenience, multiple points can be specified here. The optimization method is given as a string such that the \texttt{Optim.jl} package does not need to be included in the script. Finally, the \texttt{bayesianupdate}-function can be used again with the known syntax, i.e. a likelihood, \texttt{UQ-Model}-array and the desired object for the method are given and a \texttt{DataFrame} is returned. The log-densities are given in the \texttt{DataFrame} as the variable \texttt{:maxval}. Below is an example for the implementation.




\begin{minted}{julia}
μ = 0
σ = .2

prior = RandomVariable.(Normal(μ,σ), [:x, :y])

N1 = MvNormal([-0.5, -0.5], 0.1)
N2 = MvNormal([0.5, 0.5], 0.1)

loglikelihood =
    df -> log.([0.5 * pdf(N1, collect(x)) + 0.5 * pdf(N2, collect(x)) for x in eachrow(df)])

priorFunction =
    df -> prod.([pdf(Normal(μ, σ), collect(x)) for x in eachrow(df)])

x0 = [[.1, .1],[-.1,-.1]]

MAP = MaximumAPosterioriBayesian(prior, "LBFGS", x0)
MLE = MaximumLikelihoodBayesian(prior, "LBFGS", x0)

mapestimate = bayesianupdating(loglikelihood, UQModel[], MAP)
mlestimate = bayesianupdating(loglikelihood, UQModel[], MLE)

contour!(xs, ys, likelihood_eval.*prior_eval/.05, lim = [-2,2], c = :black)
scatter!(mapestimate.x, mapestimate.y; lim=[-2, 2], label = "MAP estimate", c = :black)
scatter!(mlestimate.x, mlestimate.y; lim=[-2,2], label = "ML estimate", c = :red)
plot!([0,0],[0,0],c = :red, label = "Likelihood")
plot!([0,0],[0,0],c = :blue, label = "Prior")
plot!([0,0],[0,0],c = :black, label = "Posterior")
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
┌ Warning: Multiple series with different color share a colorbar.
│ Colorbar may not reflect all series correctly.
└ @ Plots A:\julia\packages\Plots\MR7sb\src\backends\gr.jl:528
┌ Warning: Multiple series with different line color share a colorbar.
│ Colorbar may not reflect all series correctly.
└ @ Plots A:\julia\packages\Plots\MR7sb\src\backends\gr.jl:528
\end{minted}



The figure shows the (bimodal) likelihood in red and the prior distribution in blue. The difference in MAP and MLE is clearly visible, as the MLE conincides directly with the maxima of the likelihood, while MAP is shifted in the direction of the prior mean.



\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{manual/point-estimates.svg}}
\caption{Point estimates}
\end{figure}




\subsection{Bayesian calibration of computer simulations}



\label{4808027176221545699}{}


\emph{UncertaintyQuantification.jl} allows for complex computer models to be included in the Bayesian updating procedure, if for example one wishes to infer unknown model parameters from experimental data of model outputs. Several models can be evaluated in order to compute the likelihood function, by passing a vector of \hyperlinkref{13839731607710581418}{\texttt{UQModel}}s to the \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}} method. These will be executed before the likelihood is evaluated and models outputs will be available in the \texttt{DataFrame} inside the likelihood function. There is no restriction on the type of \hyperlinkref{13839731607710581418}{\texttt{UQModel}} used. For example, it is possible to use an \texttt{ExternalModel} and call an external solver.



For a complete example refer to the \hyperlinkref{15653452312877879055}{Inverse eigenvalue problem}.



\chapter{Parallelisation}


\section{Parallelisation}



\label{3364646822760622237}{}


\emph{UncertaintyQuantification} provides several possibilities to parallelise the execution of a model. Which method to use strongly depends on the model and the available hardware. The simplest way to run a model in parallel is by using the \hyperlinkref{1757741936032894606}{\texttt{ParallelModel}}.



\subsection{ParallelModel}



\label{1757741936032894606}{}


With the basic \hyperlinkref{12360836855591423082}{\texttt{Model}} it is up to the user to implement an efficient function which returns the model responses for all samples simultaneously. Commonly, this will involve vectorized operations. For more complex or longer running models, \emph{UncertaintyQuantification} provides a simple \hyperlinkref{1757741936032894606}{\texttt{ParallelModel}}. This model relies on the capabilites of the \texttt{Distributed} module, which is part of the standard library shipped with Julia. Without any present \emph{workers}, the \hyperlinkref{1757741936032894606}{\texttt{ParallelModel}} will evaluate its function in a loop for each sample. If one or more workers are present, it will automatically distribute the model evaluations. For this to work, \emph{UncertaintyQuantification} must be loaded with the \texttt{@everywhere} macro in order to be loaded on all workers. In difference to the standard \texttt{Model} each call to the function used in the  \texttt{ParallelModel} is passed a \texttt{DataFrameRow} instead of the full \texttt{DataFrame}.



In the following example, we first load \texttt{Distributed} and add four local workers. A simple model is then evaluated in parallel. Finally, the workers are removed.




\begin{minted}{julia}
using Distributed
addprocs(4) # add 4 local workers

# setup of the model and inputs
@everywhere begin

using UncertaintyQuantification

x = RandomVariable(Normal(), :x)
y = RandomVariable(Normal(), :y)

m = ParallelModel(df -> sqrt(df.x^2 .+ df.y^2), :z)

end

samples = sample([x, y], 1000)
evaluate!(m, samples)

rmprocs(workers()) # release the local workers
\end{minted}



It is important to note, that the \hyperlinkref{1757741936032894606}{\texttt{ParallelModel}} requires some overhead to distribute the function calls to the workers. Therefore it performs significantly slower than the standard \hyperlinkref{12360836855591423082}{\texttt{Model}} with vectorized operations for a simple function as in this example. The method of executing a model in parallel presented above also applies to the \texttt{ExternalModel}.



\begin{tcolorbox}[toptitle=-1mm,bottomtitle=1mm,colback=admonition-note!50!white,colframe=admonition-note,title=\textbf{Note}]
For heavier external models the use of parallel compute clusters through the \hyperlinkref{339239240194958289}{\texttt{SlurmInterface}} is recommended. See \hyperlinkref{957858869020788811}{High Performance Computing}.

\end{tcolorbox}


\chapter{Stochastic Dynamics}


\section{Stochastic Dynamics}



\label{8402073698242581879}{}


Stochastic dynamics focuses on the study and development of methods, applications, and techniques for analyzing systems with probabilistic behavior. Stochastic dynamic systems are mathematical models used to describe the mechanical behavior and evolution of systems that change over time and include probabilistic characteristics. These systems are characterized by their dynamics{\textquotesingle} uncertainty, variability, and randomness, which can arise from external influences, inherent randomness, or imperfect knowledge of system parameters. Such systems are crucial in fields ranging from finance and economics to engineering and natural sciences, as they allow for modeling and predicting the behavior of complex systems under uncertainty.



Some examples that shall introduce stochastic dynamics in UQ.jl study the behaviour of buildings and systems and their changes over time when they are affected by random processes, or also called stochastic processes, such as earthquakes or wind loads. Stochastic processes are characterised by an inherent randomness which can not be described deterministically [\hyperlinkref{2227099348983167607}{25}]. To study the frequency distribution and their amplitude of a stochastic process, the so-called power spectral density (PSD) function can be defined.



\subsection{Semi-empirical PSD functions}



\label{5291485014187948009}{}


Semi-empirical PSD functions describe the distribution of power across frequencies, combining theoretical models with experimental data for improved accuracy. Well established models in earthquake engineering are, for instance, the Kanai-Tajimi PSD model [\hyperlinkref{13534374753547515349}{26}, \hyperlinkref{5778133231777751075}{27}] or the Clough-Penzien PSD model [\hyperlinkref{16593283455700884559}{28}]. The Clough-Penzien PSD model \(S^{CP}\) with frequency vector \(\omega\) reads as follows



\begin{equation*}
\begin{split}    S^{CP}(\omega, S_0, \omega_{f}, \zeta_{f}, \omega_{g}, \zeta_{g}) = S_0 \cdot \frac{{\omega^4}}{{(\omega_{f}^2-\omega^2)^2+4  \zeta_{f}^2  \omega_{f}^2  \omega^2}} \cdot \frac{{\omega_{g}^4+4  \zeta_{g}^2  \omega_{g}^2  \omega^2}}{{(\omega_{g}^2-\omega^2)^2+4  \zeta_{g}^2  \omega_{g}^2  \omega^2}},\end{split}\end{equation*}


where the parameters \(S_0\), \(\omega_{f}\), \(\zeta_{f}\), \(\omega_{g}\) and \(\zeta_{g}\) characterize the soil conditions.



\subsection{Stochastic Process Generation}



\label{15268098057762990786}{}


The spectral representation method (SRM) [\hyperlinkref{17514431937910934166}{29}] can be utilised to generate realisations of stochastic processes \(x(t)\) which are carrying the characteristics of a PSD function \(S(\omega)\) in time domain. The stochastic character of these processes is represented by random variables \(\varphi\). The spectral representation method reads as follows



\begin{equation*}
\begin{split}    x(t) = \sqrt{2} \sum \limits_{n=0}^{N_{\omega}-1}\sqrt{2 S(\omega_n) \Delta \omega} \cos(\omega_n t + \varphi),\end{split}\end{equation*}


where \(\Delta \omega\) is the frequency increment, \(\omega_n\) is the frequency at coordinate \(n\), \(t\) is the time vector and \(N_{\omega}\) is the total number of frequency points.



Stochastic processes generated by SRM can be utilised in a Monte Carlo framework, for instance, in order to determine the structural reliability or other quantities of interest.



\subsection{PSD estimation}



\label{15969932036151766388}{}


If a time signal, such as an earthquake, is given in time domain and shall be studied for their characteristics in frequency domain, the PSD estimation can be carried out. Although rigorous mathematical relationships between stochastic processes and the PSD function exist, estimation techniques are often necessary since an exact determination would require continuous signals of infinite length, which are not practical in real-world applications.



A frequently used estimator of the stationary PSD function from time records is the periodogram [\hyperlinkref{2227099348983167607}{25}]. To transform a stationary stochastic process from time domain to frequency domain, the discrete Fourier transform (DFT) can be utilised. The periodogram is the squared absolute value of the DFT of the discrete time signal \(x\) with data point index \(n\), such as



\begin{equation*}
\begin{split}    \hat{S}_X(\omega_k) = \frac{\Delta t^2}{T} \left| \sum \limits_{n = 0}^{N_t-1} x_n e^{-2 \pi i k n / N} \right|^2,\end{split}\end{equation*}


where \(\Delta t\) is the time discretisation, \(T\) is duration of the record, \(N_t\) is the total number of data points, \(i\) is the imaginary unit and \(k\) is the integer frequency for \(\omega_k = \frac{2 \pi k}{T}\).



\subsection{Implementation}



\label{575251200882559516}{}


To follow the procedure of generating signals that approximate homogeneous gaussian processes, as presented in [\hyperlinkref{17514431937910934166}{29}], first we need to define parameters for the frequency domain of the PSD function.




\begin{minted}{julia}
N = 128                         # Number of terms
ω_u = 4π                        # Cut-off frequency
Δω = ω_u / N                    # Frequency discretisation size
ω = collect(0:Δω:(ω_u - Δω))    # Frequency discretisation vector
\end{minted}



With the discretized frequency domain in \(\omega\), let us create a PSD function simply by calling




\begin{minted}{julia}
sd = ShinozukaDeodatis(ω, 1, 1) # σ=1 b=1
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
ShinozukaDeodatis([0.0, 0.09817477042468103, 0.19634954084936207, 0.2945243112740431, 0.39269908169872414, 0.4908738521234052, 0.5890486225480862, 0.6872233929727672, 0.7853981633974483, 0.8835729338221293  …  11.584622910112362, 11.682797680537043, 11.780972450961723, 11.879147221386406, 11.977321991811086, 12.075496762235767, 12.173671532660448, 12.271846303085129, 12.370021073509811, 12.468195843934492], 1, 1, [0.0, 0.002184253480257515, 0.007920019787398558, 0.01615370039459961, 0.026032311709710227, 0.03687194105234497, 0.0481306505561695, 0.0593852446591769, 0.0703113898816206, 0.08066663983075278  …  0.00031229570199260145, 0.0002879115942576991, 0.00026539391594671387, 0.00024460337782646903, 0.0002254107425931193, 0.00020769613278054383, 0.00019134838302693383, 0.00017626443419187804, 0.0001623487669220179, 0.00014951287236894594])
\end{minted}



here \(\sigma=1\) defines the first parameter of the provided analytical PSD in [\hyperlinkref{17514431937910934166}{29}] and \(b=1\) the second parameter. There are other PSD functions predefined, such as




\begin{minted}{julia}
cp = CloughPenzien(ω, 0.1, 0.8π, 0.6, 8π, 0.6)  # S_0=0.1, ω_f=0.8π, ζ_f=0.6, ω_g=8π, ζ_g=0.6
kt = KanaiTajimi(ω, 0.25, 5, 0.75)              # S_0=.25, ω_0=5, ζ=0.75
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
KanaiTajimi([0.0, 0.09817477042468103, 0.19634954084936207, 0.2945243112740431, 0.39269908169872414, 0.4908738521234052, 0.5890486225480862, 0.6872233929727672, 0.7853981633974483, 0.8835729338221293  …  11.584622910112362, 11.682797680537043, 11.780972450961723, 11.879147221386406, 11.977321991811086, 12.075496762235767, 12.173671532660448, 12.271846303085129, 12.370021073509811, 12.468195843934492], 0.25, 5, 0.75, [0.25, 0.2501927099497609, 0.25077016954978437, 0.25173035972488217, 0.2530698878054118, 0.25478394696301443, 0.25686626093999926, 0.2593090156248634, 0.26210277951320693, 0.26523641560517924  …  0.10493221910977876, 0.10322840434412243, 0.10156370449415349, 0.09993703243995293, 0.0983473327418927, 0.09679358094694115, 0.09527478287513262, 0.09378997389167106, 0.09233821816944897, 0.09091860794614824])
\end{minted}



\texttt{CloughPenzien} and \texttt{KanaiTajimi} are semi-empirical PSD functions used to model ground motion for earthquake engineering applications, for reference see [\hyperlinkref{13534374753547515349}{26}], [\hyperlinkref{5778133231777751075}{27}] and [\hyperlinkref{16593283455700884559}{28}].



To go obtain the distribution of power over the frequencies call the \texttt{evaluate} function of the \texttt{PSD} object. To illustrate, we plot the created \texttt{ShinozukaDeodatis} power spectral density.




\begin{minted}{julia}
p = evaluate(sd)
plot(sd.ω, p; label="shinozuka")
\end{minted}



\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{manual/shinozuka.svg}}
\caption{Example of the Shinozuka power spectral density}
\end{figure}




, to the generation of signals in the time domain, we need to define the time domain.




\begin{minted}{julia}
T0 = 2π/Δω              # Total simulation time
Δt = 2π/(2*ω_u)         # Time step size
t = collect(Δt:Δt:(T0)) # Time discretisation vector
\end{minted}



With the time domain \texttt{t} and the PSD function \texttt{sd}, we can generate a \texttt{SpectralRepresentation} object




\begin{minted}{julia}
srm_obj = SpectralRepresentation(sd, t, :ShnzkSR)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
SpectralRepresentation(ShinozukaDeodatis([0.0, 0.09817477042468103, 0.19634954084936207, 0.2945243112740431, 0.39269908169872414, 0.4908738521234052, 0.5890486225480862, 0.6872233929727672, 0.7853981633974483, 0.8835729338221293  …  11.584622910112362, 11.682797680537043, 11.780972450961723, 11.879147221386406, 11.977321991811086, 12.075496762235767, 12.173671532660448, 12.271846303085129, 12.370021073509811, 12.468195843934492], 1, 1, [0.0, 0.002184253480257515, 0.007920019787398558, 0.01615370039459961, 0.026032311709710227, 0.03687194105234497, 0.0481306505561695, 0.0593852446591769, 0.0703113898816206, 0.08066663983075278  …  0.00031229570199260145, 0.0002879115942576991, 0.00026539391594671387, 0.00024460337782646903, 0.0002254107425931193, 0.00020769613278054383, 0.00019134838302693383, 0.00017626443419187804, 0.0001623487669220179, 0.00014951287236894594]), [0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5  …  61.75, 62.0, 62.25, 62.5, 62.75, 63.0, 63.25, 63.5, 63.75, 64.0], [0.0 0.0 … 0.0 0.0; 0.02454369260617026 0.04908738521234052 … 6.258641614573416 6.283185307179586; … ; 3.0925052683774528 6.1850105367549055 … 788.5888434362505 791.6813487046279; 3.117048960983623 6.234097921967246 … 794.8474850508238 797.9645340118075], 0.09817477042468103, [0.0, 0.02070934977123097, 0.03943465796445519, 0.05631848413707344, 0.0714942826486781, 0.08508694785836825, 0.09721332798276672, 0.10798270936615924, 0.11749727281829432, 0.12585252358466703  …  0.007830652443792696, 0.007518730566904699, 0.007218723816600235, 0.006930206412974308, 0.006652766027050156, 0.006386003469122759, 0.006129532376095776, 0.005882978898623371, 0.005645981388793122, 0.005418190089016056], :ShnzkSR, [:ShnzkSR_1, :ShnzkSR_2, :ShnzkSR_3, :ShnzkSR_4, :ShnzkSR_5, :ShnzkSR_6, :ShnzkSR_7, :ShnzkSR_8, :ShnzkSR_9, :ShnzkSR_10  …  :ShnzkSR_119, :ShnzkSR_120, :ShnzkSR_121, :ShnzkSR_122, :ShnzkSR_123, :ShnzkSR_124, :ShnzkSR_125, :ShnzkSR_126, :ShnzkSR_127, :ShnzkSR_128])
\end{minted}



To sample random phase angles, call




\begin{minted}{julia}
ϕ = sample(srm_obj)
\end{minted}



ϕ is a \texttt{DataFrame}, with each row containing one random sample for the \texttt{N} phase angles. To draw multiple samples at once, pass the number of samples as a second argument to the \texttt{sample} function.



Finally, to retrieve the signal corresponding to the phase angles  pass them to the \texttt{evaluate} method. The \texttt{ϕnames} property of the \texttt{SpectralRepresentation} helps to conveniently select only the pahase angles from the \texttt{DataFrame} in case additional variables are present.




\begin{minted}{julia}
x = evaluate(srm_obj, collect(ϕ[1, :]))
plot(srm_obj.time, x; label="signal")
\end{minted}



\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{manual/signal.svg}}
\caption{}
\end{figure}




\chapter{High Performance Computing}


\section{High performance computing}



\label{15324308029856110982}{}


\subsection{Slurm job arrays}



\label{3608381597528543034}{}


When sampling large simulation models, or complicated workflows, Julia{\textquotesingle}s inbuilt parallelism is sometimes insufficient. Job arrays are a useful feature of the slurm scheduler which allow you to run many similar jobs, which differ by an index (for example a sample number). This allows \texttt{UncertaintyQuantification.jl} to run heavier simulations (for example, simulations requiring multiple nodes), by offloading model sampling to an HPC machine using slurm. This way, \texttt{UncertaintyQuantification.jl} can be started on a single worker, and the HPC machine handles the rest.



For more information on job arrays, see: \href{https://slurm.schedmd.com/job\_array.html}{job arrays}.



\subsection{SlurmInterface}



\label{260344389708059720}{}


When \texttt{SlurmInterface} is passed to an \texttt{ExternalModel}, a slurm job array script is automatically generated and executed. Julia waits for this job to finish before extracting results and proceeding.




\begin{minted}{julia}
options = Dict(
    "account"=>"HPC_account_1",
    "partition"=>"CPU_partition",
    "job-name"=>"UQ_array",
    "nodes"=>"1",
    "ntasks" =>"32",
    "time"=>"01:00:00"
)

slurm = SlurmInterface(
    options;
    throttle=50,
    batchsize=200,
    extras=["load python3"]
)
\end{minted}



Here \texttt{account} is your account (provided by your HPC admin/PI), and \texttt{partition} specifies the queue that jobs will be submitted to (ask admin if unsure). \texttt{nodes} and \texttt{ntasks} are the number of nodes and CPUs that your individual simulations requires. Depending on your HPC machine, each node has a specific number of CPUs. If your application requires more CPUs than are available per node, you can use multiple nodes. Through \texttt{options} the \texttt{SlurmInterface} supports all options of \texttt{SBATCH} except for \texttt{array} since the job array is constructed dynamically.



The parameter \texttt{time} specifies the maximum time that each simulation will be run for, before being killed.



\begin{tcolorbox}[toptitle=-1mm,bottomtitle=1mm,colback=admonition-warning!50!white,colframe=admonition-warning,title=\textbf{Individual model runs VS overall batch}]
\texttt{nodes}, \texttt{ntasks}, and \texttt{time} are parameters required for each \emph{individual} model evaluation, not the entire batch. For example, if you are running a large FEM simulation that requires 100 CPUs to evaluate one sample, and your HPC machine has 50 CPUs per node, you would specify \texttt{nodes = 2} and \texttt{ntasks = 100}.

\end{tcolorbox}


\begin{tcolorbox}[toptitle=-1mm,bottomtitle=1mm,colback=admonition-note!50!white,colframe=admonition-note,title=\textbf{Compiling with MPI}]
If your model requires multiple \texttt{nodes}, it may be best to compile your application with MPI, if your model allows for it. Please check your application{\textquotesingle}s documentation for compiling with MPI.

\end{tcolorbox}


Any commands in \texttt{extras} will be executed before you model is run, for example loading any module files or data your model requires. Multiple commands can be passed: \texttt{extras = [{\textquotedbl}load python{\textquotedbl}, {\textquotedbl}python3 get\_data.py{\textquotedbl}]}.



\begin{tcolorbox}[toptitle=-1mm,bottomtitle=1mm,colback=admonition-note!50!white,colframe=admonition-note,title=\textbf{Note}]
If your \texttt{extras} command requires \texttt{{\textquotedbl}{\textquotedbl}} or \texttt{\$} symbols, they must be properly escaped as \texttt{{\textbackslash}{\textquotedbl}{\textbackslash}{\textquotedbl}} and \texttt{{\textbackslash}\$}.

\end{tcolorbox}


The job array task throttle, which is the number of samples that will be run concurrently at any given time, is specified by \texttt{throttle}. For example, when running a \texttt{MonteCarlo} simulation with 2000 samples, and \texttt{throttle = 50}, 2000 model evaluations will be run in total, but only 50 at the same time. If left empty, your scheduler{\textquotesingle}s default throttle will be used. Sometimes the scheduler limits the maximum size of a single job array. In these cases, the maximum size can be set through the \texttt{batchsize} parameter. This will separate the jobs into multiple smaller arrays.



\subsection{Testing your HPC configuration}



\label{8854577102267231979}{}


Slurm is tested \emph{only} on linux systems, not Mac or Windows. When testing \texttt{UncertaintyQuantification.jl} locally, we use a dummy function \texttt{test/test\_utilities/sbatch} to mimic an HPC scheduler.



\begin{tcolorbox}[toptitle=-1mm,bottomtitle=1mm,colback=admonition-warning!50!white,colframe=admonition-warning,title=\textbf{Testing locally on Linux}]
Certain Slurm tests may fail unless \texttt{test/test\_utilities/} is added to PATH. To do this: \texttt{export PATH=UncertaintyQuantification.jl/test/test\_utilities/:\$PATH}. Additionally, \emph{actual} slurm submissions may fail if \texttt{test/test\_utilities/sbatch} is called in place of your system installation. To find out which sbatch you{\textquotesingle}re using, call \texttt{which sbatch}.

\end{tcolorbox}


If you{\textquotesingle}d like to \textbf{actually} test the Slurm interface your HPC machine:




\begin{minted}{julia}
using Pkg
Pkg.test("UncertaintyQuantification"; test_args=["HPC", "YOUR_ACCOUNT", "YOUR_PARTITION"])
\end{minted}



or if you have a local clone, from the top directory:




\begin{minted}{julia}
julia --project
using Pkg
Pkg.test(; test_args=["HPC", "YOUR_ACCOUNT", "YOUR_PARTITION"])
\end{minted}



\texttt{YOUR\_ACCOUNT} and \texttt{YOUR\_PARTITION} should be replaced with your account and partition you wish to use for testing. This test will submit 4 slurm job arrays, of a lightweight calculation (> 1 minute per job) requiring 1 core/task each.



\subsubsection{Usage}



\label{1861020021534494089}{}


See \hyperlinkref{957858869020788811}{High Performance Computing} for a detailed example.



\part{Examples}


\chapter{External Models}


\section{External Solvers}



\label{11735303123183305173}{}


\subsection{OpenSees supported beam}



\label{15139231179555957110}{}


In this example we will perform the reliability analysis of a supported beam using the open-source finite element software \href{https://opensees.berkeley.edu/}{OpenSees}. The example definition can be found \href{https://opensees.berkeley.edu/wiki/index.php/Simply\_supported\_beam\_modeled\_with\_two\_dimensional\_solid\_elements}{here} We will be modeling the Young{\textquotesingle}s modulus of the \texttt{ElasticIsotropic} material in the OpenSees model as a \texttt{RandomVariable}.




\begin{minted}{julia}
using UncertaintyQuantification
using DelimitedFiles # required to extract the simulation output

E = RandomVariable(Normal(1000, 5), :E) # Young's modulus
\end{minted}



Source/Extra files are expected to be in this folder




\begin{minted}{julia}
sourcedir = joinpath(pwd(), "demo/models")
\end{minted}



These files will be rendere through Mustache.jl and have values injected




\begin{minted}{julia}
sourcefile = "supported-beam.tcl"
\end{minted}



Dictionary to map format Strings (Format.jl) to variables




\begin{minted}{julia}
numberformats = Dict(:E => ".8e")
\end{minted}



UQ will create subfolders in here to run the solver and store the results




\begin{minted}{julia}
workdir = joinpath(pwd(), "supported-beam")
\end{minted}



Read output file and compute maximum (absolute) displacement. The input \texttt{base} of the function used to construct the \texttt{Extractor} is the working directory for the current sample.




\begin{minted}{julia}
disp = Extractor(base -> begin
    file = joinpath(base, "displacement.out")
    data = readdlm(file, ' ')

    return maximum(abs.(data[:, 2]))
end, :disp)
\end{minted}



Define the solver




\begin{minted}{julia}
opensees = Solver(
    "OpenSees", # path to OpenSees binary, here we expect it to be available on the system PATH
    "supported-beam.tcl";
    args="", # (optional) extra arguments passed to the solver
)
\end{minted}



Put everything together to construct the external model




\begin{minted}{julia}
ext = ExternalModel(
    sourcedir, sourcefile, disp, opensees; workdir=workdir, formats=numberformats
)
\end{minted}



Run the probability of failure analysis with 1000 samples




\begin{minted}{julia}
pf, samples = probability_of_failure(ext, df -> 0.35 .- df.disp, E, MonteCarlo(1000))

println("Probability of failure: $pf")
\end{minted}



Running large simulations with an external model in series can be numerically demanding. The next example shows how to run the same example but execute the model in parallel.



\subsection{OpenSees supported beam parallel}



\label{6490822231582709877}{}


load the Distributed module




\begin{minted}{julia}
using Distributed
\end{minted}



add six local workers




\begin{minted}{julia}
addprocs(6)
\end{minted}



The setup of the example is enclosed in the @everywhere macro. This ensures, that the main process and all worker processes have loaded the necessary modules and received all defined variables and functions. Apart from this, the setup is identical to the serial example.




\begin{minted}{julia}
@everywhere begin
    using UncertaintyQuantification, DelimitedFiles

    E = RandomVariable(Normal(1000, 5), :E)

    sourcedir = joinpath(pwd(), "demo/models")

    sourcefile = "supported-beam.tcl"

    numberformats = Dict(:E => ".8e")

    workdir = joinpath(pwd(), "supported-beam")

    disp = Extractor(base -> begin
        file = joinpath(base, "displacement.out")
        data = readdlm(file, ' ')

        return maximum(abs.(data[:, 2]))
    end, :disp)

    opensees = Solver("OpenSees", "supported-beam.tcl"; args="")

    ext = ExternalModel(
        sourcedir, sourcefile, disp, opensees; workdir=workdir, formats=numberformats
    )
end
\end{minted}



The analysis is then executed on the main Julia process. When executing the model, the calls to the external solver will automatically be distributed to all available workers.




\begin{minted}{julia}
pf, samples = probability_of_failure(ext, df -> 0.35 .- df.disp, E, MonteCarlo(1000))

println("Probability of failure: $pf")
\end{minted}



\begin{quote}
\textbf{Note}

Local parallelisation will quickly reach it{\textquotesingle}s limits for large and complex models. Making use of  cluster thrugh the \texttt{SlurmInterface} is strongly recommended.

\end{quote}


{\rule{\textwidth}{1pt}}


\emph{This page was generated using \href{https://github.com/fredrikekre/Literate.jl}{Literate.jl}.}



\chapter{Metamodels}


\section{Metamodels}



\label{13382004127081062345}{}


\subsection{Design Of Experiments}



\label{5141534247305030192}{}


Design Of Experiments (DOE) offers various designs that can be used for creating a model of a given system. The core idea is to evaluate significant points of the system in order to obtain a sufficient model while keeping the effort to achieve this relatively low. Depending on the parameters, their individual importance and interconnections, different designs may be adequate.



The ones implemented here are \texttt{TwoLevelFactorial}, \texttt{FullFactorial}, \texttt{FractionalFactorial}, \texttt{CentralComposite} and \texttt{BoxBehnken}.



\subsection{Response Surface}



\label{11236368157692582757}{}


A Response Surface is a structure used for modeling.     It can be trained by providing it with evaluated points of a function.     It will then, using polynomial regression, compute a model of that function.



\subsection{Example}



\label{12204569949998619624}{}


In this example, we will model the following test function (known as Himmelblau{\textquotesingle}s function)



in the range \(x1, x2 ∈ [-5, 5]\). It is defined as



\begin{equation*}
\begin{split}f(x1, x2) = (x1^2 + x2 - 11)^2 + (x1 + x2^2 - 7)^2.\end{split}\end{equation*}




\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{examples/himmelblau.svg}}
\caption{}
\end{figure}
 At first we need to create an array of random variables, that will be used when evaluating the points that our design produces. It will also define the range of the function we want the design to fit. This is also a good time to declare the function that we are working with.




\begin{minted}{julia}
using UncertaintyQuantification

x = RandomVariable.(Uniform(-5, 5), [:x1, :x2])

himmelblau = Model(
    df -> (df.x1 .^ 2 .+ df.x2 .- 11) .^ 2 .+ (df.x1 .+ df.x2 .^ 2 .- 7) .^ 2, :y
)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Model(Main.var"#1#2"(), :y)
\end{minted}



Our next step is to chose the design we want to use and if required, set the parameters to the values we need or want. In this example, we are using a \texttt{FullFactorial} design:




\begin{minted}{julia}
design = FullFactorial([5, 5])
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
FullFactorial([5, 5], 1)
\end{minted}



After that, we call the sample function with our design. This produces a matrix containing the points of our design fitted to the range defined via the RandomVariables. Wer then evaluate the function we want to model in these points and use the resulting data to train a \texttt{ResponseSurface}. The \texttt{ResponseSurface} uses regression to fit a polynomial function to the given datapoints. That functions degree is set as an Integer in the constructor.



\begin{tcolorbox}[toptitle=-1mm,bottomtitle=1mm,colback=admonition-note!50!white,colframe=admonition-note,title=\textbf{Note}]
The choice of the degree and the design and its parameters may be crucial to obtaining a sufficient model.

\end{tcolorbox}



\begin{minted}{julia}
training_data = sample(x, design)
evaluate!(himmelblau, training_data)
rs = ResponseSurface(training_data, :y, 4)

test_data = sample(x, 1000)
evaluate!(rs, test_data)
\end{minted}



To evaluate the \texttt{ResponseSurface}use \texttt{evaluate!(rs::ResponseSurface, data::DataFrame)} with the \texttt{DataFrame} containing the points you want to evaluate.



The model in this case has an mse of about 1e-26 and looks like this in comparison to the original:





\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{examples/himmelblau-comparison.svg}}
\caption{}
\end{figure}




{\rule{\textwidth}{1pt}}


\emph{This page was generated using \href{https://github.com/fredrikekre/Literate.jl}{Literate.jl}.}



\chapter{Bayesian Updating}


\section{Bayesian Updating}



\label{16081037215275166927}{}


\subsection{Inverse eigenvalue problem}



\label{15653452312877879055}{}


The inverse eigenvalue problem is a classic engineering example. Here we will use Bayesian updating to sample from a bivariate posterior distribution describing unknown quantities of a matrix



\begin{equation*}
\begin{split}\begin{bmatrix}
 \theta_1 + \theta_2 & -\theta_2 \\
 -\theta_2 & \theta_2 \\
\end{bmatrix}\end{split}\end{equation*}


A matrix of this form can represent different problems, like the stiffness matrix describing a tuned mass damper system. In this example we assume the fixed values \(\theta_1 = 0.5\) and \(\theta_2 = 1.5\) for the variables.



The eigenvalues \(\lambda_1\) and \(\lambda_2\) of this matrix represent a physical measurable property corrupted by {\textquotedbl}noise{\textquotedbl} created for example due to environmental factors or measurement inaccuracy.



\begin{equation*}
\begin{split}\lambda_1^{noisy} = \frac{(\theta_1+2\theta_2)+\sqrt{\theta_1^2+4{\theta_2}^2}}{2} + \epsilon_1\end{split}\end{equation*}


\begin{equation*}
\begin{split}\lambda_2^{noisy} = \frac{(\theta_1+2\theta_2)-\sqrt{\theta_1^2+4{\theta_2}^2}}{2} + \epsilon_2\end{split}\end{equation*}


The {\textquotedbl}noise{\textquotedbl} terms \(\epsilon_1\) and \(\epsilon_2\) follow a Normal distribution with zero mean and standard deviations \(\sigma_1 = 1.0\) and \(\sigma_2 = 0.1\).



The synthetic {\textquotedbl}noisy{\textquotedbl} data used for the Bayesian updating procedure is given in the following table.




\begin{table}[h]
\centering
\begin{tabulary}{\linewidth}{R R}
\toprule
\(\lambda_1\) & \(\lambda_2\) \\
\toprule
1.51 & 0.33 \\
4.01 & 0.30 \\
3.16 & 0.17 \\
3.21 & 0.18 \\
2.19 & 0.32 \\
1.71 & 0.23 \\
2.73 & 0.21 \\
5.51 & 0.20 \\
1.95 & 0.11 \\
4.48 & 0.20 \\
1.43 & 0.16 \\
2.91 & 0.26 \\
3.91 & 0.23 \\
3.58 & 0.25 \\
2.62 & 0.25 \\
\bottomrule
\end{tabulary}

\end{table}



The a priori knowledge of \(\theta_1\) and \(\theta_2\) is that they take values between 0.01 and 4. The likelihood function used for this problem is a bivariate Gaussian function with a covariance matrix \(\begin{bmatrix} \sigma_1^2 & 0 \\ 0 & \sigma_2^2 \end{bmatrix}\), with off-diagonal terms equal to 0 and the diagonal terms corresponding to the variances of the respective noise terms.



\begin{equation*}
\begin{split}P(\lambda|\theta) \propto \exp \left[-\frac{1}{2}\sum_{i=1}^2\sum_{n=1}^{15} {\left(\frac{\lambda_{i,n}^{data}-\lambda_i^{model}}{\sigma_i}\right)}^2\right]\end{split}\end{equation*}


To begin the Bayesian model updating procedure we start by defining the data, the models for the eigenvalues (without the noise term) and the likelihood function.




\begin{minted}{julia}
Y = [
    1.51 0.33
    4.01 0.3
    3.16 0.27
    3.21 0.18
    2.19 0.33
    1.71 0.23
    2.73 0.21
    5.51 0.2
    1.95 0.11
    4.48 0.2
    1.43 0.16
    2.91 0.26
    3.81 0.23
    3.58 0.25
    2.62 0.25
]

λ1 = @. Model(df -> ((df.θ1 + 2 * df.θ2) + sqrt(df.θ1^2 + 4 * df.θ2^2)) / 2, :λ1)
λ2 = @. Model(df -> ((df.θ1 + 2 * df.θ2) - sqrt(df.θ1^2 + 4 * df.θ2^2)) / 2, :λ2)

σ = [1.0 0.1]
function likelihood(df)
    λ = [df.λ1 df.λ2]

    return log.(exp.([-0.5 * sum(((Y .- λ[n, :]') ./ σ) .^ 2) for n in axes(λ, 1)]))
end
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
likelihood (generic function with 1 method)
\end{minted}



We will solve this problem using the TMCMC algorithm, as well as multi-objective maximum a priori (MAP) and maximum likelihood (ML) estimates. Therefore, the next step is to define the \hyperlinkref{4023900003980544990}{\texttt{RandomVariable}} vector of the prior, followed by the objects for the estimaters (\hyperlinkref{14651864254862715180}{\texttt{TransitionalMarkovChainMonteCarlo}}). We also have to choose number of samples and burn-in for TMCMC.




\begin{minted}{julia}
prior = RandomVariable.(Uniform(0.01, 4), [:θ1, :θ2])

n = 1000
burnin = 0

x0 = [[1., 1.],[3.,.5],[2.,2.]]

tmcmc = TransitionalMarkovChainMonteCarlo(prior, n, burnin)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
TransitionalMarkovChainMonteCarlo(RandomVariable[RandomVariable(Uniform{Float64}(a=0.01, b=4.0), :θ1), RandomVariable(Uniform{Float64}(a=0.01, b=4.0), :θ2)], 1000, 0, 0.2, true)
\end{minted}



With the prior, likelihood, models and  MCMC sampler defined, the last step is to call the \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}} method.




\begin{minted}{julia}
samples, evidence = bayesianupdating(likelihood, [λ1, λ2], tmcmc)

scatter(samples.θ1, samples.θ2; lim=[0, 4], label="TMCMC", xlabel="θ1", ylabel="θ2")
\end{minted}



\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{examples/stiffness-samples.svg}}
\caption{Resulting TMCMC}
\end{figure}
  A scatter plot of the resulting samples shows convergence to two distinct regions. Unlike the transitional Markov Chain Monte Carlo algorithm, the standard Metropolis-Hastings algorithm would have only identified one of the two regions.



\subsection{Inverse eigenvalue problem with maximum likelihood and maximum a posteriori point estimation}



\label{6789376627140577471}{}


The inverse eigenvalue problem can also be solved with point estimation schemes, i.e. maximum likelihood estimate (MLE) and maximum a posteriori (MAP) estimate. Both find the maximum of either only the likelihood (MLE) or the posterior (MAP) using optimization. The main difference in both is that MLE does not use the prior information, it will only give an estimate of the most likely parameter values based on the measurements. MAP on the other hand takes into account the prior distribution and gives a weighted estimate of the most likely parameters. MAP thus can also be seen as regularization of MLE.



We will set up the problem the same way as in the MCMC example.




\begin{minted}{julia}
Y = [
    1.51 0.33
    4.01 0.3
    3.16 0.27
    3.21 0.18
    2.19 0.33
    1.71 0.23
    2.73 0.21
    5.51 0.2
    1.95 0.11
    4.48 0.2
    1.43 0.16
    2.91 0.26
    3.81 0.23
    3.58 0.25
    2.62 0.25
]

λ1 = @. Model(df -> ((df.θ1 + 2 * df.θ2) + sqrt(df.θ1^2 + 4 * df.θ2^2)) / 2, :λ1)
λ2 = @. Model(df -> ((df.θ1 + 2 * df.θ2) - sqrt(df.θ1^2 + 4 * df.θ2^2)) / 2, :λ2)

σ = [1.0 0.1]

function likelihood(df)
    λ = [df.λ1 df.λ2]

    return log.(exp.([-0.5 * sum(((Y .- λ[n, :]') ./ σ) .^ 2) for n in axes(λ, 1)]))
end
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
likelihood (generic function with 1 method)
\end{minted}



For MLE and MAP we need to define the prior, however note that in MLE the defined \hyperlinkref{4023900003980544990}{\texttt{RandomVariable}} is only used to inform the updating process of which paramters to update. The distribution will not affect the updating, as only the likelihood is taken into account. For the optimization we also need to define starting point(s). Since we know that the problem is multi-modal, we can define multiple starting points to find both modes. We also have to specify the optimization procedure, in this case we will use LBFGS. To illustrate the results of MAP and MLE, we will also solve the problem with TMCMC.




\begin{minted}{julia}
prior = RandomVariable.(Uniform(.1, 10), [:θ1, :θ2])

burnin = 0
n = 1000

x0 = [[1., 1.],[3.,.5]]

tmcmc = TransitionalMarkovChainMonteCarlo(prior, n, burnin)
MAP = MaximumAPosterioriBayesian(prior, "LBFGS", x0)
MLE = MaximumLikelihoodBayesian(prior, "LBFGS", x0)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
MaximumLikelihoodBayesian(RandomVariable[RandomVariable(Uniform{Float64}(a=0.1, b=10.0), :θ1), RandomVariable(Uniform{Float64}(a=0.1, b=10.0), :θ2)], "LBFGS", [[1.0, 1.0], [3.0, 0.5]], true, [-Inf], [Inf])
\end{minted}



With the prior, likelihood, models and  MCMC sampler defined, the last step is to call the \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}} method.




\begin{minted}{julia}
samples, evidence = bayesianupdating(likelihood, [λ1, λ2], tmcmc)
MapEstimate = bayesianupdating(likelihood, [λ1, λ2], MAP)
MLEstimate = bayesianupdating(likelihood, [λ1, λ2], MLE)

scatter(samples.θ1, samples.θ2; lim=[0, 4], label="TMCMC", xlabel="θ1", ylabel="θ2")
scatter!((MapEstimate.θ1, MapEstimate.θ2), label="MAP")
scatter!((MLEstimate.θ1, MLEstimate.θ2), label="MLE")
\end{minted}



\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{examples/stiffness-point-estimate-uniform.svg}}
\caption{Resulting point estimates}
\end{figure}
  A scatter plot of the resulting samples shows convergence to two distinct regions. Since we used a uniform prior distribution, ML and MAP estimates find the same estimates. With a different prior distribution, i.e. a standard normal centered on one of the modes, we obtain a different result:




\begin{minted}{julia}
priorθ1 = RandomVariable(Normal(.5, .5), :θ1)
priorθ2 = RandomVariable(Normal(1.5, .5), :θ2)

prior = [priorθ1, priorθ2]

burnin = 0
n = 1000

x0 = [[1., 1.],[3.,.5]]

tmcmc = TransitionalMarkovChainMonteCarlo(prior, n, burnin)
MAP = MaximumAPosterioriBayesian(prior, "LBFGS", x0)
MLE = MaximumLikelihoodBayesian(prior, "LBFGS", x0)

samples, evidence = bayesianupdating(likelihood, [λ1, λ2], tmcmc)
MapEstimate = bayesianupdating(likelihood, [λ1, λ2], MAP)
MLEstimate = bayesianupdating(likelihood, [λ1, λ2], MLE)

scatter(samples.θ1, samples.θ2; lim=[0, 4], label="TMCMC", xlabel="θ1", ylabel="θ2")
scatter!((MapEstimate.θ1, MapEstimate.θ2), label="MAP")
scatter!((MLEstimate.θ1, MLEstimate.θ2), label="MLE")
\end{minted}



\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{examples/stiffness-point-estimate-normal.svg}}
\caption{Resulting point estimates}
\end{figure}
 Some things to note: Results from MLE are the same as before, since the prior distribution is not taken into account. MCMC does not find the second mode, since it is much less likely than the first one, so the Markov chains do not converge there. MAP does find the mode since it uses optimization and therefore is able to find the local maximum. A look at the relative values between both modes show the differences in probability:




\begin{minted}{julia}
println(exp.(MLEstimate[!,:maxval]))
println(exp.(MapEstimate[!,:maxval]))
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
[5.846036389174841e-6, 5.84603638917482e-6]
[3.5711506271809677e-6, 1.0072485673125058e-10]
\end{minted}



The second mode is 4 orders of magnitude less probable than the first mode, which explains why the Markov chains do not converge there.



{\rule{\textwidth}{1pt}}


\emph{This page was generated using \href{https://github.com/fredrikekre/Literate.jl}{Literate.jl}.}



\chapter{Stochastic Dynamics}


\section{Stochastic Dynamics Example}



\label{6683717966732647304}{}


\subsection{OpenSees earthquake signal on a cantilever column}



\label{11825675392460849965}{}


In this example we will perform the reliability analysis of a multi element cantilever column subjected to artificial stochastic ground motions using the open-source finite element software \href{https://opensees.berkeley.edu/}{OpenSees}. The example definition can be found \href{https://opensees.berkeley.edu/wiki/index.php?title=Time\_History\_Analysis\_of\_a\_2D\_Elastic\_Cantilever\_Column}{here} A stochastic signal is generated using the Clough-Penzien Power Spectral Density and the Spectral Representation Method. The signal is applied as uniform excitation as {\textquotedbl}ground motion{\textquotedbl} to the base of the column structure.



For parallel execution, see the example in \hyperlinkref{6490822231582709877}{OpenSees supported beam parallel}





Time discretization for the signal




\begin{minted}{julia}
Δt = Parameter(0.02, :dt)
t = collect(0:Δt.value:10)
timeSteps = Parameter(length(t), :timeSteps)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Parameter(501, :timeSteps)
\end{minted}



Frequency discretization for the Power Spectral Density Function (PSD)




\begin{minted}{julia}
ω = collect(0:0.6:150)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
251-element Vector{Float64}:
   0.0
   0.6
   1.2
   1.8
   2.4
   3.0
   3.6
   4.2
   4.8
   5.4
   ⋮
 145.2
 145.8
 146.4
 147.0
 147.6
 148.2
 148.8
 149.4
 150.0
\end{minted}



Definition of Clough Penzien PSD with prescribed parameters




\begin{minted}{julia}
cp = CloughPenzien(ω, 0.1, 0.8π, 0.6, 8π, 0.6)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
CloughPenzien([0.0, 0.6, 1.2, 1.8, 2.4, 3.0, 3.6, 4.2, 4.8, 5.4  …  144.6, 145.2, 145.8, 146.4, 147.0, 147.6, 148.2, 148.8, 149.4, 150.0], 0.1, 2.5132741228718345, 0.6, 25.132741228718345, 0.6, [0.0, 0.00033479010678994165, 0.005648397880232318, 0.027238098552746347, 0.06410222525109868, 0.09353984901109647, 0.10792641434414753, 0.11381997701363389, 0.11642527143144965, 0.1180173689656726  …  0.004514428300183395, 0.00447586463650233, 0.0044377976043132854, 0.004400218647369274, 0.004363119393980723, 0.004326491652239116, 0.004290327405384585, 0.004254618807312541, 0.004219358178214532, 0.004184538000348823])
\end{minted}



Ground motion formulation using the Spectral Representation Method




\begin{minted}{julia}
gm = SpectralRepresentation(cp, t, :gm)
gm_model = StochasticProcessModel(gm)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
StochasticProcessModel(SpectralRepresentation(CloughPenzien([0.0, 0.6, 1.2, 1.8, 2.4, 3.0, 3.6, 4.2, 4.8, 5.4  …  144.6, 145.2, 145.8, 146.4, 147.0, 147.6, 148.2, 148.8, 149.4, 150.0], 0.1, 2.5132741228718345, 0.6, 25.132741228718345, 0.6, [0.0, 0.00033479010678994165, 0.005648397880232318, 0.027238098552746347, 0.06410222525109868, 0.09353984901109647, 0.10792641434414753, 0.11381997701363389, 0.11642527143144965, 0.1180173689656726  …  0.004514428300183395, 0.00447586463650233, 0.0044377976043132854, 0.004400218647369274, 0.004363119393980723, 0.004326491652239116, 0.004290327405384585, 0.004254618807312541, 0.004219358178214532, 0.004184538000348823]), [0.0, 0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18  …  9.82, 9.84, 9.86, 9.88, 9.9, 9.92, 9.94, 9.96, 9.98, 10.0], [0.0 0.0 … 0.0 0.0; 0.0 0.012 … 5.988 6.0; … ; 0.0 2.988 … 1491.0120000000002 1494.0; 0.0 3.0 … 1497.0 1500.0], 0.6, [0.0, 0.020043655558503543, 0.08232908025891447, 0.18079191979537032, 0.27734936506384583, 0.33503405619924037, 0.3598773363425058, 0.3695726889481427, 0.3737784446938314, 0.3763254479287935  …  0.0736024045817803, 0.07328736292023882, 0.07297504453699184, 0.07266541389714318, 0.07235843608575898, 0.07205407679435591, 0.07175230230774134, 0.07145307949119513, 0.07115637577798238, 0.0708621591571876], :gm, [:gm_1, :gm_2, :gm_3, :gm_4, :gm_5, :gm_6, :gm_7, :gm_8, :gm_9, :gm_10  …  :gm_242, :gm_243, :gm_244, :gm_245, :gm_246, :gm_247, :gm_248, :gm_249, :gm_250, :gm_251]), :gm)
\end{minted}



Source/Extra files are expected to be in this folder, here the injection file ground-motion.dat is located




\begin{minted}{julia}
sourcedir = joinpath(pwd(), "demo/models/opensees-dynamic-cantilever-column")
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
"A:\\src\\github.com\\FriesischScott\\UncertaintyQuantification.jl\\docs\\build\\examples\\demo/models/opensees-dynamic-cantilever-column"
\end{minted}



These files will be rendere through Mustach.jl and have values injected, for this example only ground-motion.dat will have time serieses injected




\begin{minted}{julia}
sourcefile = ["cantilever-column.tcl", "ground-motion.dat"]
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
2-element Vector{String}:
 "cantilever-column.tcl"
 "ground-motion.dat"
\end{minted}



Dictionary to map format Strings (Formatting.jl) to variables




\begin{minted}{julia}
numberformats = Dict(:dt => ".8e", :gm => ".8e")
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Dict{Symbol, String} with 2 entries:
  :dt => ".8e"
  :gm => ".8e"
\end{minted}



UQ will create subfolders in here to run the solver and store the results




\begin{minted}{julia}
workdir = joinpath(pwd(), "workdir-cantilever-column")
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
"A:\\src\\github.com\\FriesischScott\\UncertaintyQuantification.jl\\docs\\build\\examples\\workdir-cantilever-column"
\end{minted}



Read output file and compute maximum (absolute) displacement An extractor is based the working directory for the current sample




\begin{minted}{julia}
max_abs_disp = Extractor(base -> begin
    file = joinpath(base, "displacement.out")
    data = DelimitedFiles.readdlm(file, ' ')

    return maximum(abs.(data[:, 2]))
end, :max_abs_disp)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Extractor(Main.var"#1#2"(), :max_abs_disp)
\end{minted}



Extractor for the full time series of the displacement at the top node




\begin{minted}{julia}
disp = Extractor(base -> begin
    file = joinpath(base, "displacement.out")
    data = DelimitedFiles.readdlm(file, ' ')

    return data[:, 2]
end, :disp)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Extractor(Main.var"#3#4"(), :disp)
\end{minted}



Extractor for the simulation time




\begin{minted}{julia}
sim_time = Extractor(base -> begin
    file = joinpath(base, "displacement.out")
    data = DelimitedFiles.readdlm(file, ' ')

    return data[:, 1]
end, :sim_time)


opensees = Solver(
    "OpenSees", # path to OpenSees binary
    "cantilever-column.tcl";
    args="", # (optional) extra arguments passed to the solver
)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Solver("OpenSees", "cantilever-column.tcl", "")
\end{minted}



Define the external model with all needed parameters and attributes




\begin{minted}{julia}
ext = ExternalModel(
    sourcedir, sourcefile, [max_abs_disp, disp, sim_time], opensees; workdir=workdir, formats=numberformats
)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
ExternalModel("A:\\src\\github.com\\FriesischScott\\UncertaintyQuantification.jl\\docs\\build\\examples\\demo/models/opensees-dynamic-cantilever-column", ["cantilever-column.tcl", "ground-motion.dat"], Extractor[Extractor(Main.var"#1#2"(), :max_abs_disp), Extractor(Main.var"#3#4"(), :disp), Extractor(Main.var"#5#6"(), :sim_time)], Solver("OpenSees", "cantilever-column.tcl", ""), "A:\\src\\github.com\\FriesischScott\\UncertaintyQuantification.jl\\docs\\build\\examples\\workdir-cantilever-column", String[], Dict(:dt => ".8e", :gm => ".8e"), false, nothing)
\end{minted}



Define the UQ.jl models used in the analysis




\begin{minted}{julia}
models = [gm_model, ext]
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
2-element Vector{UQModel}:
 StochasticProcessModel(SpectralRepresentation(CloughPenzien([0.0, 0.6, 1.2, 1.8, 2.4, 3.0, 3.6, 4.2, 4.8, 5.4  …  144.6, 145.2, 145.8, 146.4, 147.0, 147.6, 148.2, 148.8, 149.4, 150.0], 0.1, 2.5132741228718345, 0.6, 25.132741228718345, 0.6, [0.0, 0.00033479010678994165, 0.005648397880232318, 0.027238098552746347, 0.06410222525109868, 0.09353984901109647, 0.10792641434414753, 0.11381997701363389, 0.11642527143144965, 0.1180173689656726  …  0.004514428300183395, 0.00447586463650233, 0.0044377976043132854, 0.004400218647369274, 0.004363119393980723, 0.004326491652239116, 0.004290327405384585, 0.004254618807312541, 0.004219358178214532, 0.004184538000348823]), [0.0, 0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18  …  9.82, 9.84, 9.86, 9.88, 9.9, 9.92, 9.94, 9.96, 9.98, 10.0], [0.0 0.0 … 0.0 0.0; 0.0 0.012 … 5.988 6.0; … ; 0.0 2.988 … 1491.0120000000002 1494.0; 0.0 3.0 … 1497.0 1500.0], 0.6, [0.0, 0.020043655558503543, 0.08232908025891447, 0.18079191979537032, 0.27734936506384583, 0.33503405619924037, 0.3598773363425058, 0.3695726889481427, 0.3737784446938314, 0.3763254479287935  …  0.0736024045817803, 0.07328736292023882, 0.07297504453699184, 0.07266541389714318, 0.07235843608575898, 0.07205407679435591, 0.07175230230774134, 0.07145307949119513, 0.07115637577798238, 0.0708621591571876], :gm, [:gm_1, :gm_2, :gm_3, :gm_4, :gm_5, :gm_6, :gm_7, :gm_8, :gm_9, :gm_10  …  :gm_242, :gm_243, :gm_244, :gm_245, :gm_246, :gm_247, :gm_248, :gm_249, :gm_250, :gm_251]), :gm)
 ExternalModel("A:\\src\\github.com\\FriesischScott\\UncertaintyQuantification.jl\\docs\\build\\examples\\demo/models/opensees-dynamic-cantilever-column", ["cantilever-column.tcl", "ground-motion.dat"], Extractor[Extractor(Main.var"#1#2"(), :max_abs_disp), Extractor(Main.var"#3#4"(), :disp), Extractor(Main.var"#5#6"(), :sim_time)], Solver("OpenSees", "cantilever-column.tcl", ""), "A:\\src\\github.com\\FriesischScott\\UncertaintyQuantification.jl\\docs\\build\\examples\\workdir-cantilever-column", String[], Dict(:dt => ".8e", :gm => ".8e"), false, nothing)
\end{minted}



Simple Monte Carlo simulation with 1000 samples to estimate a failure probability (should be roughly around 10{\textasciicircum}-2)




\begin{minted}{julia}
pf, mc_std, samples = probability_of_failure(models, df -> 200 .- df.max_abs_disp, [Δt, timeSteps, gm], MonteCarlo(100))
\end{minted}



Plotting of single time history




\begin{minted}{julia}
plot(t, samples.gm[1]./(maximum(abs.(samples.gm[1]))); label="Stochastic ground motion acceleration", xlabel="time in s", ylabel="Normalized acceleration and displacement")
plot!(samples.sim_time[1], samples.disp[1]./(maximum(abs.(samples.disp[1]))); label="Displacement at top node", linewidth=2)
\end{minted}



\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{assets/time-history.svg}}
\caption{Resulting time history}
\end{figure}
 A plot to visualize the stochastic input ground motion acceleration singal and the resulting displacement time series at the top node of the cantilever column.



\subsection{Signal Analysis}



\label{2703783626997087689}{}


\subsubsection{First passage analysis of stochastic signals generated from a stochastic process approximated by the spectral representation method}



\label{6920089283529822742}{}


In this example we will perform a first passage analysis of a stochastic process generated from a spectral representation model. The spectral representation method is a technique to approximate a stochastic process by a linear combination of sinusoidal functions. It is used to generate stochastic signals which can, for example, represent the ground motion of an earthquake.



First passage analysis is a method to estimate the probability of a limit state being exceeded by a stochastic process. The limit state function is usually of the structure {\textquotedbl}limit state = capacity - demand{\textquotedbl}, where the capacity is a constant value and the demand is the maximum absolute value of the stochastic signals. The probability of failure is then estimated by the fraction of the number of times the capacity is exceeded by the demand. For more information on the theory see \hyperlinkref{5425193432845275192}{Reliability-Analysis}.



For parallel execution, see the example in \hyperlinkref{6490822231582709877}{OpenSees supported beam parallel}





Frequency discretization for the Power Spectral Density Function (PSD)




\begin{minted}{julia}
ω = collect(0:0.6:150)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
251-element Vector{Float64}:
   0.0
   0.6
   1.2
   1.8
   2.4
   3.0
   3.6
   4.2
   4.8
   5.4
   ⋮
 145.2
 145.8
 146.4
 147.0
 147.6
 148.2
 148.8
 149.4
 150.0
\end{minted}



Definition of Clough Penzien PSD with prescribed parameters




\begin{minted}{julia}
cp_psd = CloughPenzien(ω, 0.1, 0.8π, 0.6, 8π, 0.6)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
CloughPenzien([0.0, 0.6, 1.2, 1.8, 2.4, 3.0, 3.6, 4.2, 4.8, 5.4  …  144.6, 145.2, 145.8, 146.4, 147.0, 147.6, 148.2, 148.8, 149.4, 150.0], 0.1, 2.5132741228718345, 0.6, 25.132741228718345, 0.6, [0.0, 0.00033479010678994165, 0.005648397880232318, 0.027238098552746347, 0.06410222525109868, 0.09353984901109647, 0.10792641434414753, 0.11381997701363389, 0.11642527143144965, 0.1180173689656726  …  0.004514428300183395, 0.00447586463650233, 0.0044377976043132854, 0.004400218647369274, 0.004363119393980723, 0.004326491652239116, 0.004290327405384585, 0.004254618807312541, 0.004219358178214532, 0.004184538000348823])
\end{minted}



Ground motion model




\begin{minted}{julia}
gm = SpectralRepresentation(cp_psd, collect(0:0.02:10), :gm)
gm_model = StochasticProcessModel(gm)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
StochasticProcessModel(SpectralRepresentation(CloughPenzien([0.0, 0.6, 1.2, 1.8, 2.4, 3.0, 3.6, 4.2, 4.8, 5.4  …  144.6, 145.2, 145.8, 146.4, 147.0, 147.6, 148.2, 148.8, 149.4, 150.0], 0.1, 2.5132741228718345, 0.6, 25.132741228718345, 0.6, [0.0, 0.00033479010678994165, 0.005648397880232318, 0.027238098552746347, 0.06410222525109868, 0.09353984901109647, 0.10792641434414753, 0.11381997701363389, 0.11642527143144965, 0.1180173689656726  …  0.004514428300183395, 0.00447586463650233, 0.0044377976043132854, 0.004400218647369274, 0.004363119393980723, 0.004326491652239116, 0.004290327405384585, 0.004254618807312541, 0.004219358178214532, 0.004184538000348823]), [0.0, 0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18  …  9.82, 9.84, 9.86, 9.88, 9.9, 9.92, 9.94, 9.96, 9.98, 10.0], [0.0 0.0 … 0.0 0.0; 0.0 0.012 … 5.988 6.0; … ; 0.0 2.988 … 1491.0120000000002 1494.0; 0.0 3.0 … 1497.0 1500.0], 0.6, [0.0, 0.020043655558503543, 0.08232908025891447, 0.18079191979537032, 0.27734936506384583, 0.33503405619924037, 0.3598773363425058, 0.3695726889481427, 0.3737784446938314, 0.3763254479287935  …  0.0736024045817803, 0.07328736292023882, 0.07297504453699184, 0.07266541389714318, 0.07235843608575898, 0.07205407679435591, 0.07175230230774134, 0.07145307949119513, 0.07115637577798238, 0.0708621591571876], :gm, [:gm_1, :gm_2, :gm_3, :gm_4, :gm_5, :gm_6, :gm_7, :gm_8, :gm_9, :gm_10  …  :gm_242, :gm_243, :gm_244, :gm_245, :gm_246, :gm_247, :gm_248, :gm_249, :gm_250, :gm_251]), :gm)
\end{minted}



Capacity value for the limit state function




\begin{minted}{julia}
capacity = Parameter(21, :cap) #estimation for a capacity value which for this gm_model results in pf ~ [1.0e-6, 2.0e-5]
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
Parameter(21, :cap)
\end{minted}



Limit state function




\begin{minted}{julia}
function limitstate(df)
    return df.cap - map(x -> maximum(abs.(x)), df.gm)
end

models = [gm_model]
inputs = [gm, capacity]
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
2-element Vector{UQInput}:
 SpectralRepresentation(CloughPenzien([0.0, 0.6, 1.2, 1.8, 2.4, 3.0, 3.6, 4.2, 4.8, 5.4  …  144.6, 145.2, 145.8, 146.4, 147.0, 147.6, 148.2, 148.8, 149.4, 150.0], 0.1, 2.5132741228718345, 0.6, 25.132741228718345, 0.6, [0.0, 0.00033479010678994165, 0.005648397880232318, 0.027238098552746347, 0.06410222525109868, 0.09353984901109647, 0.10792641434414753, 0.11381997701363389, 0.11642527143144965, 0.1180173689656726  …  0.004514428300183395, 0.00447586463650233, 0.0044377976043132854, 0.004400218647369274, 0.004363119393980723, 0.004326491652239116, 0.004290327405384585, 0.004254618807312541, 0.004219358178214532, 0.004184538000348823]), [0.0, 0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18  …  9.82, 9.84, 9.86, 9.88, 9.9, 9.92, 9.94, 9.96, 9.98, 10.0], [0.0 0.0 … 0.0 0.0; 0.0 0.012 … 5.988 6.0; … ; 0.0 2.988 … 1491.0120000000002 1494.0; 0.0 3.0 … 1497.0 1500.0], 0.6, [0.0, 0.020043655558503543, 0.08232908025891447, 0.18079191979537032, 0.27734936506384583, 0.33503405619924037, 0.3598773363425058, 0.3695726889481427, 0.3737784446938314, 0.3763254479287935  …  0.0736024045817803, 0.07328736292023882, 0.07297504453699184, 0.07266541389714318, 0.07235843608575898, 0.07205407679435591, 0.07175230230774134, 0.07145307949119513, 0.07115637577798238, 0.0708621591571876], :gm, [:gm_1, :gm_2, :gm_3, :gm_4, :gm_5, :gm_6, :gm_7, :gm_8, :gm_9, :gm_10  …  :gm_242, :gm_243, :gm_244, :gm_245, :gm_246, :gm_247, :gm_248, :gm_249, :gm_250, :gm_251])
 Parameter(21, :cap)
\end{minted}



Compute probability of failure using standard Monte Carlo




\begin{minted}{julia}
mc = MonteCarlo(10^6)
\end{minted}


\begin{minted}[xleftmargin=-\fboxsep,xrightmargin=-\fboxsep,bgcolor=white,frame=single]{text}
MonteCarlo(1000000)
\end{minted}



Simple Monte Carlo simulation with 10{\textasciicircum}6 samples to estimate a failure probability (\(pf \approx [1.0e-6, 2.0e-5]\))




\begin{minted}{julia}
mc_pf, mc_std, mc_samples = probability_of_failure(models, limitstate, inputs, mc)
\end{minted}



Compute probability of failure using Subset Sampling




\begin{minted}{julia}
subset = UncertaintyQuantification.SubSetSimulation(2000, 0.1, 10, Uniform(-0.5, 0.5))
subset_pf, subset_std, subset_samples = probability_of_failure(models, limitstate, inputs, subset)
\end{minted}



Compute probability of failure using conditional Subset Sampling




\begin{minted}{julia}
subset_inf = UncertaintyQuantification.SubSetInfinity(2000, 0.1, 10, 0.5)
subset_pf_inf, subset_std_inf, subset_samples_inf = probability_of_failure(models, limitstate, inputs, subset_inf)
\end{minted}



Compute probability of failure using adaptive Subset Sampling




\begin{minted}{julia}
subset_adap = UncertaintyQuantification.SubSetInfinityAdaptive(2000, 0.1, 10, 10, 0.6, 1.0)
subset_pf_adap, subset_std_adap, subset_samples_adap = probability_of_failure(models, limitstate, inputs, subset_adap)
\end{minted}



{\rule{\textwidth}{1pt}}


\emph{This page was generated using \href{https://github.com/fredrikekre/Literate.jl}{Literate.jl}.}



\chapter{High Performance Computing}


\section{High Performance Computing}



\label{957858869020788811}{}


\subsection{OpenMC TBR uncertainty}



\label{3995829456779924398}{}


In this example, we will run \href{https://openmc.org/}{OpenMC}, to compute the tritium breeding ratio (TBR) uncertainty, by varying material and geometric properties. This example was taken from the \href{https://github.com/fusion-energy/neutronics-workshop}{Fusion Neutronics Workshop}.



At first we need to create an array of random variables, that will be used when evaluating the points that our design produces.It will also define the range of the function we want the design to fit. This is also a good time to declare the function that we are working with.



Here we will vary the model{\textquotesingle}s Li6 enrichment, and the radius of the tungsten layer.




\begin{minted}{julia}
using UncertaintyQuantification

E = RandomVariable(Uniform(0.3, 0.7), :E)
R1 = RandomVariable(Uniform(8, 14), :R1)
\end{minted}



Source/Extra files are expected to be in this folder.




\begin{minted}{julia}
sourcedir = joinpath(pwd(), "demo/models")
\end{minted}



These files will be rendered through Mustache.jl and have values injected.




\begin{minted}{julia}
sourcefile = "openmc_TBR.py"
\end{minted}



Dictionary to map format Strings (Format.jl) to variables.




\begin{minted}{julia}
numberformats = Dict(:E => ".8e")
\end{minted}



UQ will create subfolders in here to run the solver and store the results.




\begin{minted}{julia}
workdir = joinpath(pwd(), "openmc_TBR")
\end{minted}



\begin{quote}
\textbf{Note}

If Slurm is to run the jobs on multiple nodes, all the above folders must be shared by the computing nodes.

\end{quote}


Read output file and compute the Tritium breeding ratio. An extractor is based the working directory for the current sample.




\begin{minted}{julia}
TBR = Extractor(base -> begin
    file = joinpath(base, "openmc.out")
    line = readline(file)
    tbr = parse(Float64, split(line, " ")[1])

    return tbr
end, :TBR)
\end{minted}



In this example, an OpenMC model is built and run using the Python API. We therefore specify the \texttt{python3}` command, and the python file to run.




\begin{minted}{julia}
openmc = Solver(
    "python3", # path to python3 binary
    "openmc_TBR.py";
    args="", # (optional) extra arguments passed to the solver
)
\end{minted}



slurm  sbatch options




\begin{minted}{julia}
options = Dict(
    "job-name" => "UQ_slurm",
    "account" => "EXAMPLE-0001-CPU",
    "partition" => "cpu_partition",
    "nodes" => "1",
    "ntasks" => "1",
    "time" => "00:05:00",
)
\end{minted}



Slurm interface, passing required machine information. Note in \texttt{extras}, we specify the commands we require to run the model (for example, loading modulefiles or data).




\begin{minted}{julia}
slurm = SlurmInterface(
    options;
    throttle=50,
    extras=["module load openmc", "source ~/.virtualenvs/openmc/bin/activate"],
)
\end{minted}



With the \texttt{SlurmInterface} defined we can assemble the \texttt{ExternalModel}.




\begin{minted}{julia}
ext = ExternalModel(
    sourcedir,
    sourcefile,
    TBR,
    openmc;
    workdir=workdir,
    formats=numberformats,
    scheduler=slurm,
)
\end{minted}



Specify a limitstate function, negative value consititutes failure. Here we are interested in P(TBR <= 1).




\begin{minted}{julia}
function limitstate(df)
    return reduce(vcat, df.TBR) .- 1
end
\end{minted}



Finally, we can run a Monte Carlo simulation to obtain the probability of failure.




\begin{minted}{julia}
@time pf, σ, samples = probability_of_failure(ext, limitstate, [E, R1], MonteCarlo(5000))
\end{minted}



The simulation results in \texttt{pf = 0.0064} and \texttt{σ = 0.001127} for \texttt{5000} samples. Also \texttt{TBR mean = 1.2404114576444423}, \texttt{TBR std = 0.10000460056126671}, and \texttt{TBR 95\%: [1.0379178446904211, 1.4130216792418262]}.



We can also obtain the probability of failure with Subset simulation.




\begin{minted}{julia}
subset = SubSetInfinity(800, 0.1, 10, 0.5)

@time pf, std, samples = probability_of_failure(ext, limitstate, [E, R1], subset)
println("Probability of failure: $pf")
\end{minted}



This results in \texttt{pf = 0.0053} and \texttt{σ = 0.001133} for \texttt{2400} samples.



{\rule{\textwidth}{1pt}}


\emph{This page was generated using \href{https://github.com/fredrikekre/Literate.jl}{Literate.jl}.}



\part{Benchmarks}


\chapter{Subset Simulation}


\section{High dimensional Subset simulation}



\label{14614691805620732692}{}


\subsection{Subset simulation}



\label{12765450934100348318}{}


The implemented subset simulation algorithms \hyperlinkref{4173351192588427739}{\texttt{SubSetSimulation}} (using Metropolis-Hastings MCMC), \hyperlinkref{18397467037730927218}{\texttt{SubSetInfinity}} (conditional sampling MCMC) and \hyperlinkref{532901089789295354}{\texttt{SubSetInfinityAdaptive}} (adaptive conditional sampling MCMC), work efficiently in high dimensions. This benchmark shows how these algorithms scale with increasing number of dimension \texttt{N} and increasingly smaller target probability of failure \texttt{pf\_target}.



\subsection{Example function}



\label{15523964065255424817}{}


In this example, the test model will be sum of independent standard normal distributions



\begin{equation*}
\begin{split}f_N(X) = \sum^N_i X_i,\end{split}\end{equation*}


where \(X_i \sim \Phi(0, 1)\) are standard normal random variables. We will define a linear limitstate



\begin{equation*}
\begin{split}g_N(X) = C_N - f_N(X),\end{split}\end{equation*}


where \(C_N\) will be defined such that the failure probability \(\mathbb{P}(g(X) \leq 0)\) matches a pre-defined value \texttt{pf\_target}.



We can find \(C_N\) analytically, depending on the chosen number of dimensions and target probability of failure



\begin{equation*}
\begin{split}C_N = F_{\Phi_{\sqrt{N}}}^{-1}(1 - p_{\text{target}}),\end{split}\end{equation*}


where \(F_{\Phi_{\sqrt{N}}}^{-1}\) is the quantile function of a Gaussian distribution, with zero mean and standard deviation \texttt{sqrt(N)}.



Since the dimension and failure probability are two parameters of this numerical experiment, we can dynamically create the required number of random variables using broadcasting




\begin{minted}{julia}
using UncertaintyQuantification

N = 2000

inputs = RandomVariable.(Normal(), [Symbol("x$i") for i in 1:N])

\end{minted}



The model can be defined generalized for arbitrary dimensions by summing the columns of the \texttt{DataFrame}. Using \texttt{names(inputs)} to select the columns we can safely exclude any extra variables that might be present.




\begin{minted}{julia}
f = Model(
    df -> sum(eachcol(df[:, names(inputs)])),
    :f
)
\end{minted}



Next, the \texttt{pf\_target} and corresponding limit state are defined.




\begin{minted}{julia}
pf_target = 1e-9

fail_limit = quantile(Normal(0, sqrt(N)), 1 - pf_target)

function g(df)
    return fail_limit .- df.f
end
\end{minted}



For this benchmark, the probability of failure will be estimated using all available variants of Subset simulation




\begin{minted}{julia}
subset_MH = SubSetSimulation(2000, 0.1, 20, Uniform(-0.5, 0.5))
subset_CS = SubSetInfinity(2000, 0.1, 20, 0.5)
subset_aCS = SubSetInfinityAdaptive(2000, 0.1, 20, 200)
\end{minted}



\begin{tcolorbox}[toptitle=-1mm,bottomtitle=1mm,colback=admonition-note!50!white,colframe=admonition-note,title=\textbf{Monte Carlo simulation}]
Although standard Monte Carlo simulation works independently of dimension, for a target failure probability of \(10^{-9}\), even with a billion \(10^9\) samples it can return \(p_f=0\).

\end{tcolorbox}


As a first benchmark the three subset simulation algorithms are used to solve the example problem with a fixed number of dimensions \texttt{N=200} and sample size per level \texttt{N\_samples=2000} for increasingly smaller target probability of failures. The following figure shows the average estimated probabilities of failure and the standard deviation resulting from 100 independent simulation runs. Note how the variance of the estimation increases for smaller \texttt{pf} values. However, in comparison, the variance of the Metropolis-Hastings variant is higher than the variance of the conditional sampling methods.



\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{assets/subset-pf.svg}}
\caption{Subset simulation for smaller pfs}
\end{figure}




For the next benchmark, the number of dimensions remains fixed at 200 while the  number of samples is increased to estimate a target probability of failure of \(10^{-4}\).



\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{assets/subset-samples.svg}}
\caption{Subset simulation with increasing samples}
\end{figure}




The final benchmark uses the same fixed target \(p_{f} = 10^{-4}\) and again keeps the number of samples constant at \texttt{2000} per level. This time, the number of dimensions is increased to raise the complexity of the problem.



\begin{figure}
\centering
%\adjustbox{max width=\linewidth}{\includesvg{assets/subset-dimensions.svg}}
\caption{Subset simulation with increasing dimensions}
\end{figure}




\part{API}


\chapter{Inputs}


\section{Inputs}



\label{509063468412031875}{}


\subsection{Index}



\label{6663683553518785561}{}

\begin{itemize}
\item \hyperlinkref{18426214934190206718}{\texttt{UncertaintyQuantification.EmpiricalDistribution}}
\item \hyperlinkref{16181946445135277992}{\texttt{UncertaintyQuantification.Interval}}
\item \hyperlinkref{16741877178295688592}{\texttt{UncertaintyQuantification.Parameter}}
\item \hyperlinkref{460210473987211826}{\texttt{UncertaintyQuantification.ProbabilityBox}}
\item \hyperlinkref{4023900003980544990}{\texttt{UncertaintyQuantification.RandomVariable}}
\item \hyperlinkref{9310057082002087846}{\texttt{UncertaintyQuantification.sample}}
\item \hyperlinkref{9310057082002087846}{\texttt{UncertaintyQuantification.sample}}
\end{itemize}


\subsection{Types}



\label{12125088992129103176}{}

\hypertarget{16741877178295688592}{\texttt{UncertaintyQuantification.Parameter}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
Parameter(value::Real, name::Symbol)
\end{minted}

Defines a parameter value (scalar), with an input value and a name.

\textbf{Examples}


\begin{minted}{jlcon}
julia> Parameter(3.14, :π)
Parameter(3.14, :π)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/parameter.jl#L1-L11}{\texttt{source}}


\end{adjustwidth}
\hypertarget{4023900003980544990}{\texttt{UncertaintyQuantification.RandomVariable}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
RandomVariable(dist::UnivariateDistribution, name::Symbol)
\end{minted}

Defines a random variable, with a univariate distribution from Distributions.jl and a name.

\textbf{Examples}


\begin{minted}{jlcon}
julia> RandomVariable(Normal(), :x)
RandomVariable(Normal{Float64}(μ=0.0, σ=1.0), :x)

julia> RandomVariable(Exponential(1), :x)
RandomVariable(Exponential{Float64}(θ=1.0), :x)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/randomvariables/randomvariable.jl#L1-L15}{\texttt{source}}


\end{adjustwidth}
\hypertarget{18426214934190206718}{\texttt{UncertaintyQuantification.EmpiricalDistribution}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
EmpiricalDistribution(x::Vector{<:Real}, n::Integer=10000)

Creates an empirical distribution from the data given in `x` using kernel density estimation.
The kernel used is Gaussian and the bandwidth is obtained through the Sheather-Jones method.
The support is inferred from the kde using numerical root finding.
The `cdf` and `quantile` functions are linearly interpolated using `n` data points.
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/empiricaldistribution.jl#L1-L8}{\texttt{source}}


\end{adjustwidth}
\hypertarget{16181946445135277992}{\texttt{UncertaintyQuantification.Interval}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
Interval(lb::Real, up::real, name::Symbol)
\end{minted}

Defines an Interval, with lower a bound, an upper bound and a name.

\textbf{Examples}

\texttt{jldoctest julia> Interval(0.10, 0.14, :b) Interval(0.1, 0.14, :b)}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/imprecise/interval.jl#L1-L11}{\texttt{source}}


\end{adjustwidth}
\hypertarget{460210473987211826}{\texttt{UncertaintyQuantification.ProbabilityBox}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
ProbabilityBox{T}(p::AbstractVector{Interval}, name::Symbol)
\end{minted}

Defines an ProbabilityBox from a \texttt{Vector} of \texttt{Interval}, name \texttt{UnivariateDistribution} \texttt{T}. The number and order of parameters must match the parameters of the associated distribution from Distributions.jl.

\textbf{Examples}


\begin{minted}{jlcon}
julia>  ProbabilityBox{Uniform}([Interval(1.75, 1.83, :a), Interval(1.77, 1.85, :b)], :l)
ProbabilityBox{Uniform}(Interval[Interval(1.75, 1.83, :a), Interval(1.77, 1.85, :b)], :l, 1.75, 1.85)
\end{minted}


\begin{minted}{jlcon}
julia>  ProbabilityBox{Normal}([Interval(0, 1, :μ), Interval(0.1, 1, :σ)], :x)
ProbabilityBox{Normal}(Interval[Interval(0, 1, :μ), Interval(0.1, 1, :σ)], :x, -Inf, Inf)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/imprecise/p-box.jl#L1-L17}{\texttt{source}}


\end{adjustwidth}

\subsection{Functions}



\label{13536066633202303496}{}

\hypertarget{9310057082002087846}{\texttt{UncertaintyQuantification.sample}}  -- {Function.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
sample(rv::RandomVariable, n::Integer=1)
\end{minted}

Generates n samples from a random variable. Returns a DataFrame.

\textbf{Examples}

See also: \hyperlinkref{4023900003980544990}{\texttt{RandomVariable}}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/randomvariables/randomvariable.jl#L21-L29}{\texttt{source}}


\end{adjustwidth}
\hypertarget{964858933162579450}{\texttt{UncertaintyQuantification.sample}}  -- {Function.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
sample(inputs::Vector{<:UQInput}, n::Integer=1)
\end{minted}

Generates n correlated samples from a collection of inputs. Returns a DataFrame

See also: \hyperlinkref{4023900003980544990}{\texttt{RandomVariable}}, \hyperlinkref{16741877178295688592}{\texttt{Parameter}}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/inputs.jl#L1-L7}{\texttt{source}}


\end{adjustwidth}

\chapter{Models}


\section{Models}



\label{3607757037139922397}{}


\subsection{Index}



\label{6663683553518785561}{}

\begin{itemize}
\item \hyperlinkref{10696406664176468523}{\texttt{UncertaintyQuantification.Model}}
\item \hyperlinkref{1457344338231328694}{\texttt{UncertaintyQuantification.ParallelModel}}
\item \hyperlinkref{13839731607710581418}{\texttt{UncertaintyQuantification.UQModel}}
\item \hyperlinkref{13118330489865978658}{\texttt{UncertaintyQuantification.evaluate!}}
\item \hyperlinkref{16576970544334108695}{\texttt{UncertaintyQuantification.evaluate!}}
\end{itemize}


\subsection{Types}



\label{12125088992129103176}{}

\hypertarget{13839731607710581418}{\texttt{UncertaintyQuantification.UQModel}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}

Abstract supertype for all model types



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/UncertaintyQuantification.jl#L36-L38}{\texttt{source}}


\end{adjustwidth}
\hypertarget{10696406664176468523}{\texttt{UncertaintyQuantification.Model}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
Model(f::Function, name::Symbol)
\end{minted}

The function \texttt{f} must accept a \texttt{DataFrame} and return the result of the model for each row in the \texttt{DataFrame} as a vector. The \texttt{name} is used to add the output to the \texttt{DataFrame}.



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/models/model.jl#L1-L6}{\texttt{source}}


\end{adjustwidth}
\hypertarget{1457344338231328694}{\texttt{UncertaintyQuantification.ParallelModel}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
ParallelModel(f::Function, name::Symbol)
\end{minted}

The  \texttt{ParallelModel}  does what the \texttt{Model} does with a small difference. The function \texttt{f} is passed a \texttt{DataFrameRow} not the full \texttt{DataFrame}. If workers (through \texttt{Distributed}) are present, the rows are evaluated in parallel.



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/models/model.jl#L12-L18}{\texttt{source}}


\end{adjustwidth}

\subsection{Methods}



\label{15695795754956981461}{}

\hypertarget{13118330489865978658}{\texttt{UncertaintyQuantification.evaluate!}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
evaluate!(m::Model, df::DataFrame)
\end{minted}

Calls \texttt{m.func} with \texttt{df} and adds the result to the \texttt{DataFrame} as a column \texttt{m.name}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/models/model.jl#L32-L36}{\texttt{source}}


\end{adjustwidth}
\hypertarget{16576970544334108695}{\texttt{UncertaintyQuantification.evaluate!}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
evaluate!(m::ParallelModel, df::DataFrame)
\end{minted}

Calls \texttt{m.func} for each row of \texttt{df} and adds the result to the \texttt{DataFrame} as a column \texttt{m.name}. If workers are added through \texttt{Distributed}, the rows will be evaluated in parallel.



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/models/model.jl#L42-L47}{\texttt{source}}


\end{adjustwidth}

\chapter{Reliability}


\section{Reliability}



\label{10487444835713888107}{}


\subsection{Index}



\label{6663683553518785561}{}

\begin{itemize}
\item \hyperlinkref{15353394232169751668}{\texttt{UncertaintyQuantification.FORM}}
\item \hyperlinkref{13167408614513251961}{\texttt{UncertaintyQuantification.probability\_of\_failure}}
\item \hyperlinkref{6695762949520847822}{\texttt{UncertaintyQuantification.probability\_of\_failure}}
\end{itemize}


\subsection{Types}



\label{12125088992129103176}{}

\hypertarget{15353394232169751668}{\texttt{UncertaintyQuantification.FORM}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
FORM(n::Integer=10,tol::Real=1e-3,fdm::FiniteDifferencesMethod=CentralFiniteDifferences(3))
\end{minted}

used to perform the first order reliability method using the HLRF algorithm with \texttt{n} iterations and tolerance \texttt{tol}. Gradients are estimated through \texttt{fdm}.

\textbf{References}

[\hyperlinkref{9935717079447158324}{12}]



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/reliability/form.jl#L1-L9}{\texttt{source}}


\end{adjustwidth}

\subsection{Methods}



\label{15695795754956981461}{}

\hypertarget{6695762949520847822}{\texttt{UncertaintyQuantification.probability\_of\_failure}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
probability_of_failure(models::Union{Vector{<:UQModel},UQModel},performance::Function),inputs::Union{Vector{<:UQInput},UQInput},sim::FORM)
\end{minted}

Perform a reliability analysis using the first order reliability method (FORM), see \hyperlinkref{15353394232169751668}{\texttt{FORM}}. Returns the estimated probability of failure \texttt{pf}, the reliability index \texttt{β} and the design point \texttt{dp}.

\textbf{Examples}


\begin{minted}{text}
pf, β, dp = probability_of_failure(model, performance, inputs, sim)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/reliability/form.jl#L24-L34}{\texttt{source}}


\end{adjustwidth}
\hypertarget{13167408614513251961}{\texttt{UncertaintyQuantification.probability\_of\_failure}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
probability_of_failure(models::Union{Vector{<:UQModel},UQModel},performance::Function),inputs::Union{Vector{<:UQInput},UQInput},sim::AbstractMonteCarlo)
\end{minted}

Perform a reliability analysis with a standard Monte Carlo simulation. Returns the estimated probability of failure \texttt{pf}, the standard deviation \texttt{σ} and the \texttt{DataFrame} containing the evaluated \texttt{samples}. The simulation \texttt{sim} can be any instance of \texttt{AbstractMonteCarlo}.

\textbf{Examples}


\begin{minted}{text}
pf, σ, samples = probability_of_failure(model, performance, inputs, sim)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/reliability/probabilityoffailure.jl#L1-L12}{\texttt{source}}


\end{adjustwidth}

\chapter{ResponseSurface}


\section{ResponseSurface}



\label{4297740406374411039}{}


\subsection{Index}



\label{6663683553518785561}{}



\subsection{Type}



\label{5754456517017253880}{}

\hypertarget{1355353364404016589}{\texttt{UncertaintyQuantification.ResponseSurface}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
ResponseSurface(data::DataFrame, dependendVarName::Symbol, deg::Int, dim::Int)
\end{minted}

Creates a response surface using polynomial least squares regression with given degree.

\textbf{Examples}


\begin{minted}{jlcon}
julia> data = DataFrame(x = 1:10, y = [1, 4, 10, 15, 24, 37, 50, 62, 80, 101]);

julia> rs = ResponseSurface(data, :y, 2)
ResponseSurface([0.48333333333332457, -0.23863636363636026, 1.0189393939393936], :y, [:x], 2, Monomials.Monomial[1, x1, x1²])
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/models/responsesurface.jl#L1-L13}{\texttt{source}}


\end{adjustwidth}

\subsection{Functions}



\label{13536066633202303496}{}

\hypertarget{10749969932520960523}{\texttt{UncertaintyQuantification.evaluate!}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
evaluate!(rs::ResponseSurface, data::DataFrame)
\end{minted}

evaluating data by using a previously trained ResponseSurface.

\textbf{Examples}


\begin{minted}{jlcon}
julia> data = DataFrame(x = 1:10, y = [1, 4, 10, 15, 24, 37, 50, 62, 80, 101]);

julia> rs = ResponseSurface(data, :y, 2);

julia> df = DataFrame(x = [2.5, 11, 15]);

julia> evaluate!(rs, df);

julia> df.y |> DisplayAs.withcontext(:compact => true)
3-element Vector{Float64}:
   6.25511
 121.15
 226.165
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/models/responsesurface.jl#L39-L62}{\texttt{source}}


\end{adjustwidth}

\chapter{PolyharmonicSpline}


\section{PolyharmonicSpline}



\label{14740625054619865040}{}


\subsection{Index}



\label{6663683553518785561}{}

\begin{itemize}
\item \hyperlinkref{12360404346968240388}{\texttt{UncertaintyQuantification.PolyharmonicSpline}}
\item \hyperlinkref{14940947651413538197}{\texttt{UncertaintyQuantification.evaluate!}}
\end{itemize}


\subsection{Type}



\label{5754456517017253880}{}

\hypertarget{12360404346968240388}{\texttt{UncertaintyQuantification.PolyharmonicSpline}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
PolyharmonicSpline(data::DataFrame, k::Int64, output::Symbol)
\end{minted}

Creates a polyharmonic spline that is trained by given data.

\#Examples


\begin{minted}{jlcon}
julia> data = DataFrame(x = 1:10, y = [1, -5, -10, -12, -8, -1, 5, 12, 23, 50]);

julia> PolyharmonicSpline(data, 2, :y) |> DisplayAs.withcontext(:compact => true)
PolyharmonicSpline([1.14733, -0.449609, 0.0140379, -1.02859, -0.219204, 0.900367, 0.00895592, 1.07145, -5.33101, 3.88628], [-112.005, 6.84443], [1.0; 2.0; … ; 9.0; 10.0;;], 2, [:x], :y)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/models/polyharmonicspline.jl#L1-L13}{\texttt{source}}


\end{adjustwidth}

\subsection{Functions}



\label{13536066633202303496}{}

\hypertarget{14940947651413538197}{\texttt{UncertaintyQuantification.evaluate!}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
evaluate!(ps::PolyharmonicSpline, df::DataFrame)
\end{minted}

Evaluate given data using a previously contructed PolyharmonicSpline metamodel.

\#Examples


\begin{minted}{jlcon}
julia> data = DataFrame(x = 1:10, y = [1, -5, -10, -12, -8, -1, 5, 12, 23, 50]);

julia> ps = PolyharmonicSpline(data, 2, :y);

julia> df = DataFrame( x = [2.5, 7.5, 12, 30]);

julia> evaluate!(ps, df);

julia> df.y |> DisplayAs.withcontext(:compact => true)
4-element Vector{Float64}:
  -7.75427
   8.29083
  84.4685
 260.437
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/models/polyharmonicspline.jl#L70-L92}{\texttt{source}}


\end{adjustwidth}

\chapter{Simulations}


\section{Simulations}



\label{782416527003373519}{}


Various Monte Carlo based simulations for a wide range of applications.



\subsection{Index}



\label{6663683553518785561}{}

\begin{itemize}
\item \hyperlinkref{4916152994621835685}{\texttt{UncertaintyQuantification.RadialBasedImportanceSampling}}
\item \hyperlinkref{18397467037730927218}{\texttt{UncertaintyQuantification.SubSetInfinity}}
\item \hyperlinkref{532901089789295354}{\texttt{UncertaintyQuantification.SubSetInfinityAdaptive}}
\item \hyperlinkref{4173351192588427739}{\texttt{UncertaintyQuantification.SubSetSimulation}}
\end{itemize}


\subsection{Types}



\label{12125088992129103176}{}

\hypertarget{4916152994621835685}{\texttt{UncertaintyQuantification.RadialBasedImportanceSampling}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
RadialBasedImportanceSampling(n::Integer, β::Real)
\end{minted}

Used to perform radial-based importance sampling with \texttt{n} samples and reliability index \texttt{β}. If no \texttt{β} or \texttt{β=0.0} is passed, a \hyperlinkref{15353394232169751668}{\texttt{FORM}} analysis will automatically be performed to estimate the reliability index.

\textbf{Examples}

\texttt{jldoctest julia> rbis = RadialBasedImportanceSampling(1000) RadialBasedImportanceSampling(10000, 0.0)}`

\textbf{References}

[\hyperlinkref{8183389152391755847}{14}]



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/simulations/radialbasedimportancesampling.jl#L1-L18}{\texttt{source}}


\end{adjustwidth}
\hypertarget{4173351192588427739}{\texttt{UncertaintyQuantification.SubSetSimulation}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
SubSetSimulation(n::Integer, target::Float64, levels::Integer, proposal::UnivariateDistribution)
\end{minted}

Defines the properties of a Subset simulation where \texttt{n} is the number of initial samples, \texttt{target} is the target probability of failure at each level, \texttt{levels} is the maximum number of levels and \texttt{proposal} is the proposal distribution for the markov chain monte carlo.

\textbf{Examples}


\begin{minted}{jlcon}
julia> SubSetSimulation(100, 0.1, 10, Uniform(-0.2, 0.2))
SubSetSimulation(100, 0.1, 10, Uniform{Float64}(a=-0.2, b=0.2))
\end{minted}

\textbf{References}

[\hyperlinkref{15932307010839829307}{17}]



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/simulations/subset.jl#L3-L20}{\texttt{source}}


\end{adjustwidth}
\hypertarget{18397467037730927218}{\texttt{UncertaintyQuantification.SubSetInfinity}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
SubSetInfinity(n::Integer, target::Float64, levels::Integer, s::Real)
\end{minted}

Defines the properties of a Subset-∞ simulation where \texttt{n} is the number of initial samples, \texttt{target} is the target probability of failure at each level, \texttt{levels} is the maximum number of levels and \texttt{s} is the standard deviation for the proposal samples.

\textbf{Examples}


\begin{minted}{jlcon}
julia> SubSetInfinity(100, 0.1, 10, 0.5)
SubSetInfinity(100, 0.1, 10, 0.5)
\end{minted}

\textbf{References}

[\hyperlinkref{345953961655055560}{18}]

[\hyperlinkref{17411345754570781475}{30}]



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/simulations/subset.jl#L43-L62}{\texttt{source}}


\end{adjustwidth}
\hypertarget{532901089789295354}{\texttt{UncertaintyQuantification.SubSetInfinityAdaptive}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
SubSetInfinityAdaptive(n::Integer, target::Float64, levels::Integer, Na::Integer, λ::Real, s::Real)
\end{minted}

Implementation of: Papaioannou, Iason, et al. {\textquotedbl}MCMC algorithms for subset simulation.{\textquotedbl} Probabilistic Engineering Mechanics 41 (2015): 89-103

Defines the properties of a Subset-∞ adaptive where \texttt{n} are the number of samples per level, \texttt{target} is the target probability of failure at each level, \texttt{levels} is the maximum number of levels, \texttt{λ} (λ = 1 recommended) is the initial scaling parameter, and \texttt{Na} is the number simulations that will be run before \texttt{λ} is updated. Note that Na must be a multiple of n * target: \texttt{mod(ceil(n * target), Na) == 0)}. The initial variance of the proposal distribution is \texttt{s}.

Idea behind this algorithm is to adaptively select the correlation parameter of \texttt{s} at each intermediate level, by simulating a subset Na of the chains (which must be choosen without replacement at random) and modifying the acceptance rate towards the optimal αstar = 0.44

\textbf{Constructors}

\begin{itemize}
\item \texttt{SubSetInfinityAdaptive(n::Integer, target::Float64, levels::Integer, Na::Integer)}   (default: λ = s = 1)


\item \texttt{SubSetInfinityAdaptive(n::Integer, target::Float64, levels::Integer, Na::Integer, λ::Real)} (λ = s)


\item \texttt{SubSetInfinityAdaptive(n::Integer, target::Float64, levels::Integer, Na::Integer, λ::Real, s::Real)}

\end{itemize}
\textbf{Note}

The following constructors will run the same number of samples, but SubSetInfinityAdaptive will update \texttt{s} after each chain:

\begin{itemize}
\item \texttt{SubSetInfinityAdaptive(400, 0.1, 10, 40)}


\item \texttt{SubSetInfinity(400, 0.1, 10, 0.5)}

\end{itemize}
\textbf{Examples}


\begin{minted}{jlcon}
julia> SubSetInfinityAdaptive(200, 0.1, 10, 2)
SubSetInfinityAdaptive(200, 0.1, 10, 2, 1, 1)
\end{minted}

\textbf{References}

[\hyperlinkref{8907925646719725278}{31}]

[\hyperlinkref{2393110712994691496}{32}]



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/simulations/subset.jl#L92-L132}{\texttt{source}}


\end{adjustwidth}

\chapter{Bayesian Updating}


\section{Bayesian Updating}



\label{16081037215275166927}{}


Methods for Bayesian updating.



\subsection{Index}



\label{6663683553518785561}{}

\begin{itemize}
\item \hyperlinkref{8639565537460690979}{\texttt{UncertaintyQuantification.AbstractBayesianMethod}}
\item \hyperlinkref{4470365816705975422}{\texttt{UncertaintyQuantification.AbstractBayesianPointEstimate}}
\item \hyperlinkref{7557579786475836974}{\texttt{UncertaintyQuantification.MaximumAPosterioriBayesian}}
\item \hyperlinkref{7464647358232787048}{\texttt{UncertaintyQuantification.MaximumLikelihoodBayesian}}
\item \hyperlinkref{17074782294394699701}{\texttt{UncertaintyQuantification.SingleComponentMetropolisHastings}}
\item \hyperlinkref{14651864254862715180}{\texttt{UncertaintyQuantification.TransitionalMarkovChainMonteCarlo}}
\item \hyperlinkref{741170815147723344}{\texttt{UncertaintyQuantification.bayesianupdating}}
\end{itemize}


\subsection{Types}



\label{12125088992129103176}{}

\hypertarget{8639565537460690979}{\texttt{UncertaintyQuantification.AbstractBayesianMethod}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
AbstractBayesianMethod
\end{minted}

Subtypes are used to dispatch to the differenct MCMC methods in \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}}.

Subtypes are:

\begin{itemize}
\item \hyperlinkref{17074782294394699701}{\texttt{SingleComponentMetropolisHastings}}


\item \hyperlinkref{14651864254862715180}{\texttt{TransitionalMarkovChainMonteCarlo}}

\end{itemize}


\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/UncertaintyQuantification.jl#L47-L56}{\texttt{source}}


\end{adjustwidth}
\hypertarget{17074782294394699701}{\texttt{UncertaintyQuantification.SingleComponentMetropolisHastings}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
SingleComponentMetropolisHastings(proposal, x0, n, burnin, islog)
\end{minted}

Passed to \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}} to run the single-component Metropolis-Hastings algorithm starting from \texttt{x0} with  univariate proposal distibution \texttt{proposal}. Will generate \texttt{n} samples \emph{after} performing \texttt{burnin} steps of the Markov chain and discarding the samples. The flag \texttt{islog} specifies whether the prior and likelihood functions passed to the  \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}} method are already  given as logarithms.

Alternative constructor


\begin{minted}{julia}
    SingleComponentMetropolisHastings(proposal, x0, n, burnin)  # `islog` = true
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/modelupdating/bayesianupdating.jl#L1-L12}{\texttt{source}}


\end{adjustwidth}
\hypertarget{14651864254862715180}{\texttt{UncertaintyQuantification.TransitionalMarkovChainMonteCarlo}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
TransitionalMarkovChainMonteCarlo(prior, n, burnin, β, islog)

Passed to [`bayesianupdating`](@ref) to run thetransitional Markov chain Monte Carlo algorithm  with [`RandomVariable'](@ref) vector `prior`. At each transitional level, one sample will be generated from `n` independent Markov chains after `burnin` steps have been discarded. The flag `islog` specifies whether the prior and likelihood functions passed to the  [`bayesianupdating`](@ref) method are already  given as logarithms.
\end{minted}

Alternative constructors


\begin{minted}{julia}
    TransitionalMarkovChainMonteCarlo(prior, n, burnin, β)  # `islog` = true
     TransitionalMarkovChainMonteCarlo(prior, n, burnin)    # `β` = 0.2,  `islog` = true
\end{minted}

\textbf{References}

[\hyperlinkref{7619550157776039859}{24}]



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/modelupdating/bayesianupdating.jl#L134-L150}{\texttt{source}}


\end{adjustwidth}
\hypertarget{4470365816705975422}{\texttt{UncertaintyQuantification.AbstractBayesianPointEstimate}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
AbstractBayesianPointEstimate
\end{minted}

Subtypes are used to dispatch to the differenct point estimation methods in \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}}.

Subtypes are:

\begin{itemize}
\item \hyperlinkref{7557579786475836974}{\texttt{MaximumAPosterioriBayesian}}


\item \hyperlinkref{7464647358232787048}{\texttt{MaximumLikelihoodBayesian}}

\end{itemize}


\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/UncertaintyQuantification.jl#L59-L68}{\texttt{source}}


\end{adjustwidth}
\hypertarget{7557579786475836974}{\texttt{UncertaintyQuantification.MaximumAPosterioriBayesian}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
MaximumAPosterioriBayesian(prior, optimmethod, x0; islog, lowerbounds, upperbounds)
\end{minted}

Passed to \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}} to estimate one or more maxima of the posterior distribution starting from \texttt{x0}. The optimization uses the method specified in \texttt{optimmethod}. Will calculate one estimation per point in x0. The flag \texttt{islog} specifies whether the prior and likelihood functions passed to the  \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}} method are already  given as logarithms. \texttt{lowerbounds} and \texttt{upperbounds} specify optimization intervals.

Alternative constructors


\begin{minted}{julia}
    MaximumAPosterioriBayesian(prior, optimmethod, x0; islog) # `lowerbounds` = [-Inf], # `upperbounds` = [Inf]
    MaximumAPosterioriBayesian(prior, optimmethod, x0)  # `islog` = true
\end{minted}

See also \hyperlinkref{7464647358232787048}{\texttt{MaximumLikelihoodBayesian}}, \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}},  \hyperlinkref{14651864254862715180}{\texttt{TransitionalMarkovChainMonteCarlo}}.



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/modelupdating/bayesianMAP.jl#L1-L13}{\texttt{source}}


\end{adjustwidth}
\hypertarget{7464647358232787048}{\texttt{UncertaintyQuantification.MaximumLikelihoodBayesian}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
MaximumLikelihoodBayesian(prior, optimmethod, x0; islog, lowerbounds, upperbounds)
\end{minted}

Passed to \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}} to estimate one or more maxima of the likelihood starting from \texttt{x0}. The optimization uses the method specified in \texttt{optimmethod}. Will calculate one estimation per point in x0. The flag \texttt{islog} specifies whether the prior and likelihood functions passed to the  \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}} method are already  given as logarithms. \texttt{lowerbounds} and \texttt{upperbounds} specify optimization intervals.

Alternative constructors


\begin{minted}{julia}
    MaximumLikelihoodBayesian(prior, optimmethod, x0; islog) # `lowerbounds` = [-Inf], # `upperbounds` = [Inf]
    MaximumLikelihoodBayesian(prior, optimmethod, x0)  # `islog` = true
\end{minted}

\textbf{Notes}

The method uses \texttt{prior} only as information on which parameters are supposed to be optimized. The prior itself does not influence the result of the maximum likelihood estimate and can be given as a dummy distribution. For example, if two parameters \texttt{a} and \texttt{b} are supposed to be optimized, the prior could look like this


\begin{minted}{julia}
    prior = RandomVariable.(Uniform(0,1), [:a, :b])
\end{minted}

See also \hyperlinkref{7557579786475836974}{\texttt{MaximumAPosterioriBayesian}}, \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}},  \hyperlinkref{14651864254862715180}{\texttt{TransitionalMarkovChainMonteCarlo}}.



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/modelupdating/bayesianMAP.jl#L52-L70}{\texttt{source}}


\end{adjustwidth}

\subsection{Methods}



\label{15695795754956981461}{}

\hypertarget{741170815147723344}{\texttt{UncertaintyQuantification.bayesianupdating}}  -- {Function.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
bayesianupdating(likelihood, models, pointestimate; prior)
\end{minted}

Perform bayesian updating using the given \texttt{likelihood}, \texttt{models}  and any point estimation method \hyperlinkref{4470365816705975422}{\texttt{AbstractBayesianPointEstimate}}.

\textbf{Notes}

Method can be called with an empty Vector of models, i.e.


\begin{minted}{text}
bayesianupdating(likelihood, [], pointestimate)
\end{minted}

If \texttt{prior} is not given, the method will construct a prior distribution from the prior specified in \texttt{AbstractBayesianPointEstimate.prior}.

\texttt{likelihood} is a Julia function which must be defined in terms of a \texttt{DataFrame} of samples, and must evaluate the likelihood for each row of the \texttt{DataFrame}

For example, a loglikelihood based on normal distribution using {\textquotesingle}Data{\textquotesingle}:


\begin{minted}{julia}
likelihood(df) = [sum(logpdf.(Normal.(df_i.x, 1), Data)) for df_i in eachrow(df)]
\end{minted}

If a model evaluation is required to evaluate the likelihood, a vector of \texttt{UQModel}s must be passed to \texttt{bayesianupdating}. For example if the variable \texttt{x} above is the output of a numerical model.

For a general overview of the function, see \hyperlinkref{741170815147723344}{\texttt{bayesianupdating}}.



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/modelupdating/bayesianMAP.jl#L112-L136}{\texttt{source}}



\begin{minted}{julia}
bayesianupdating(prior, likelihood, models, mcmc)
\end{minted}

Perform bayesian updating using the given \texttt{prior}, \texttt{likelihood}, \texttt{models}  and any MCMC sampler \hyperlinkref{8639565537460690979}{\texttt{AbstractBayesianMethod}}.

Alternatively the method can be called without \texttt{models}.


\begin{minted}{text}
bayesianupdating(prior, likelihood, mcmc)
\end{minted}

When using \hyperlinkref{14651864254862715180}{\texttt{TransitionalMarkovChainMonteCarlo}} the \texttt{prior} can automatically be constructed.


\begin{minted}{text}
bayesinupdating(likelihood, models, tmcmc)
bayesianupdating(likelihood, tmcmc)
\end{minted}

\textbf{Notes}

\texttt{likelihood} is a Julia function which must be defined in terms of a \texttt{DataFrame} of samples, and must evaluate the likelihood for each row of the \texttt{DataFrame}

For example, a loglikelihood based on normal distribution using {\textquotesingle}Data{\textquotesingle}:


\begin{minted}{julia}
likelihood(df) = [sum(logpdf.(Normal.(df_i.x, 1), Data)) for df_i in eachrow(df)]
\end{minted}

If a model evaluation is required to evaluate the likelihood, a vector of \texttt{UQModel}s must be passed to \texttt{bayesianupdating}. For example if the variable \texttt{x} above is the output of a numerical model.



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/modelupdating/bayesianupdating.jl#L34-L61}{\texttt{source}}


\end{adjustwidth}

\chapter{Power Spectral Density Functions}


\section{Power Spectral Density Functions}



\label{5658933604860497984}{}


Construction and evaluation of different Power Spectral Density (PSD) functions. Correpsonding theory and literature can be found here \hyperlinkref{5291485014187948009}{Semi-empirical-PSD-functions}.



\subsection{Index}



\label{6663683553518785561}{}

\begin{itemize}
\item \hyperlinkref{18058881860453401202}{\texttt{UncertaintyQuantification.CloughPenzien}}
\item \hyperlinkref{11283129380918957935}{\texttt{UncertaintyQuantification.EmpiricalPSD}}
\item \hyperlinkref{7921530830185962688}{\texttt{UncertaintyQuantification.KanaiTajimi}}
\item \hyperlinkref{698434710585563404}{\texttt{UncertaintyQuantification.ShinozukaDeodatis}}
\end{itemize}


\subsection{Types of PSD functions}



\label{11019922910080293649}{}

\hypertarget{18058881860453401202}{\texttt{UncertaintyQuantification.CloughPenzien}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
CloughPenzien(ω::AbstractVector{<:Real}, S_0::Real, ω_f::Real, ζ_f::Real, ω_g::Real, ζ_g::Real)
\end{minted}

Constructs a \texttt{CloughPenzien} instance representing a power spectral density function with the given parameters.

\textbf{Arguments / Parameters}

\begin{itemize}
\item \texttt{ω::AbstractVector\{<:Real\}}: A vector of angular frequencies.


\item \texttt{S\_0::Real}: A scaling factor.


\item \texttt{ω\_f::Real}: Frequency parameter for the first oscillator.


\item \texttt{ζ\_f::Real}: Damping ratio for the first oscillator.


\item \texttt{ω\_g::Real}: Frequency parameter for the second oscillator.


\item \texttt{ζ\_g::Real}: Damping ratio for the second oscillator.

\end{itemize}
\textbf{Returns}

A discretized \texttt{CloughPenzien} power spectral density function specified by given arguments (parameters).

\textbf{Example}


\begin{minted}{julia}
w = 0:0.1:10
S_0 = 1.0
ω_f = 2.0
ζ_f = 0.05
ω_g = 3.0
ζ_g = 0.1
cp_psd = CloughPenzien(w, S_0, ω_f, ζ_f, ω_g, ζ_g)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/dynamics/psd.jl#L13-L39}{\texttt{source}}


\end{adjustwidth}
\hypertarget{7921530830185962688}{\texttt{UncertaintyQuantification.KanaiTajimi}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
KanaiTajimi(ω::AbstractVector{<:Real}, S_0::Real, ω_0::Real, ζ::Real) -> KanaiTajimi
\end{minted}

Constructs a \texttt{KanaiTajimi} instance representing a power spectral density function with the given parameters.

\textbf{Arguments}

\begin{itemize}
\item \texttt{ω::AbstractVector\{<:Real\}}: A vector of angular frequencies.


\item \texttt{S\_0::Real}: A scaling factor.


\item \texttt{ω\_0::Real}: Natural frequency of the oscillator.


\item \texttt{ζ::Real}: Damping ratio of the oscillator.

\end{itemize}
\textbf{Returns}

A discretized \texttt{KanaiTajimi} power spectral density function specified by given arguments (parameters).

\textbf{Example}


\begin{minted}{julia}
w = 0:0.1:10
S_0 = 1.0
ω_0 = 2.0
ζ = 0.05
kt = KanaiTajimi(w, S_0, ω_0, ζ)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/dynamics/psd.jl#L60-L82}{\texttt{source}}


\end{adjustwidth}
\hypertarget{698434710585563404}{\texttt{UncertaintyQuantification.ShinozukaDeodatis}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
ShinozukaDeodatis(ω::AbstractVector{<:Real}, σ::Real, b::Real)
\end{minted}

Constructs a \texttt{ShinozukaDeodatis} instance representing a power spectral density function with the given parameters.

\textbf{Arguments}

\begin{itemize}
\item \texttt{ω::AbstractVector\{<:Real\}}: A vector of angular frequencies.


\item \texttt{σ::Real}: A hyperparamter related to the variance of the stochastic process.


\item \texttt{b::Real}: A parameter related to the correlation length of the stochastic process.

\end{itemize}
\textbf{Returns}

A discretized \texttt{ShinozukaDeodatis} instance with the power spectral density function specified by given arguments (parameters).

\textbf{Example}


\begin{minted}{julia}
w = 0:0.1:10
σ = 1.0
b = 0.5
sd = ShinozukaDeodatis(w, σ, b)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/dynamics/psd.jl#L109-L129}{\texttt{source}}


\end{adjustwidth}
\hypertarget{11283129380918957935}{\texttt{UncertaintyQuantification.EmpiricalPSD}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
EmpiricalPSD(ω::AbstractVector{<:Real}, p::AbstractVector{<:Real}) -> EmpiricalPSD
\end{minted}

Constructs an \texttt{EmpiricalPSD} instance with the given angular frequencies and manually provided power spectral density values.

\textbf{Arguments}

\begin{itemize}
\item \texttt{ω::AbstractVector\{<:Real\}}: A vector of angular frequencies.


\item \texttt{p::AbstractVector\{<:Real\}}: A vector of power spectral density values corresponding to the frequencies in \texttt{ω}.

\end{itemize}
\textbf{Returns}

A discretized \texttt{EmpiricalPSD} instance with manually pre-specified provided power spectral density values.

\textbf{Example}


\begin{minted}{julia}
w = 0:0.1:10
p_values = rand(length(w))  # Example empirical PSD values
emp_psd = EmpiricalPSD(w, p_values)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/dynamics/psd.jl#L139-L157}{\texttt{source}}


\end{adjustwidth}

\chapter{Stochastic Processes (Spectral Representation)}


\section{Stochastic Processes (Spectral Representation)}



\label{11408399352375379803}{}


Stochastic process generation based on the Spectral Representation Method which utilizes Power Spectral Density Functions. Correpsonding theory and literature can be found here \hyperlinkref{15268098057762990786}{Stochastic-Process-Generation}.



\subsection{Index}



\label{6663683553518785561}{}

\begin{itemize}
\item \hyperlinkref{10048673472492320317}{\texttt{UncertaintyQuantification.SpectralRepresentation}}
\item \hyperlinkref{9389867473198091497}{\texttt{Base.names}}
\item \hyperlinkref{17794587236484596438}{\texttt{UncertaintyQuantification.dimensions}}
\item \hyperlinkref{9137458819988012181}{\texttt{UncertaintyQuantification.evaluate}}
\item \hyperlinkref{9310057082002087846}{\texttt{UncertaintyQuantification.sample}}
\item \hyperlinkref{8606567393101569031}{\texttt{UncertaintyQuantification.to\_physical\_space!}}
\item \hyperlinkref{1433719112274392831}{\texttt{UncertaintyQuantification.to\_standard\_normal\_space!}}
\end{itemize}


\subsection{Types and Spectral Representation functions}



\label{18058198324837651359}{}

\hypertarget{10048673472492320317}{\texttt{UncertaintyQuantification.SpectralRepresentation}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
SpectralRepresentation(psd::AbstractPowerSpectralDensity, time::AbstractVector{<:Real}, name::Symbol) -> SpectralRepresentation
\end{minted}

Constructs a \texttt{SpectralRepresentation} instance representing a stochastic process generated using the spectral representation method.

\textbf{Arguments}

\begin{itemize}
\item \texttt{psd::AbstractPowerSpectralDensity}: An instance of a power spectral density model.


\item \texttt{time::AbstractVector\{<:Real\}}: A vector of time points.


\item \texttt{name::Symbol}: A symbol representing the name of the process.

\end{itemize}
\textbf{Returns}

A \texttt{SpectralRepresentation} instance with the given arguments (parameters).

\textbf{Example}


\begin{minted}{julia}
w = 0:0.1:10
S_0 = 1.0
ω_f = 2.0
ζ_f = 0.05
ω_g = 3.0
ζ_g = 0.1
psd = CloughPenzien(w, S_0, ω_f, ζ_f, ω_g, ζ_g)
t = 0:0.1:10
name = :process1
sr = SpectralRepresentation(psd, t, name)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/stochasticprocesses/spectralrepresentation.jl#L13-L40}{\texttt{source}}


\end{adjustwidth}
\hypertarget{9310057082002087846}{\texttt{UncertaintyQuantification.sample}}  -- {Function.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
sample(sr::SpectralRepresentation, n::Integer=1) -> DataFrame
\end{minted}

Generates samples of random phase angles for a given \texttt{SpectralRepresentation} instance.

\textbf{Arguments}

\begin{itemize}
\item \texttt{sr::SpectralRepresentation}: An instance of the \texttt{SpectralRepresentation} struct.


\item \texttt{n::Integer=1}: The number of samples to generate (default is 1).

\end{itemize}
\textbf{Returns}

A \texttt{DataFrame} containing the generated samples of random phase angles.

\textbf{Example}


\begin{minted}{julia}
w = 0:0.1:10
psd = CloughPenzien(w, 1.0, 2.0, 0.05, 3.0, 0.1)
t = 0:0.1:10
name = :process1
sr = SpectralRepresentation(psd, t, name)
samples = sample(sr, 5)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/stochasticprocesses/spectralrepresentation.jl#L60-L82}{\texttt{source}}


\end{adjustwidth}
\hypertarget{9137458819988012181}{\texttt{UncertaintyQuantification.evaluate}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
evaluate(sr::SpectralRepresentation, ϕ::AbstractVector{<:Real}) -> AbstractVector{<:Real}
\end{minted}

Evaluates the stochastic process for a given \texttt{SpectralRepresentation} instance and a vector of random phase angles.

\textbf{Arguments}

\begin{itemize}
\item \texttt{sr::SpectralRepresentation}: An instance of the \texttt{SpectralRepresentation} struct.


\item \texttt{ϕ::AbstractVector\{<:Real\}}: A vector of random phase angles.

\end{itemize}
\textbf{Returns}

A vector of real numbers representing the evaluated stochastic process.

\textbf{Example}


\begin{minted}{julia}
w = 0:0.1:10
psd = CloughPenzien(w, 1.0, 2.0, 0.05, 3.0, 0.1)
t = 0:0.1:10
name = :process1
sr = SpectralRepresentation(psd, t, name)
ϕ = rand(Uniform(0, 2π), length(psd.ω))
process_values = evaluate(sr, ϕ)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/stochasticprocesses/spectralrepresentation.jl#L87-L110}{\texttt{source}}


\end{adjustwidth}
\hypertarget{1433719112274392831}{\texttt{UncertaintyQuantification.to\_standard\_normal\_space!}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
to_standard_normal_space!(sr::SpectralRepresentation, df::DataFrame) -> Nothing
\end{minted}

Transforms the random phase angles in the given \texttt{DataFrame} from a uniform distribution to a standard normal distribution.

\textbf{Arguments}

\begin{itemize}
\item \texttt{sr::SpectralRepresentation}: An instance of the \texttt{SpectralRepresentation} struct.


\item \texttt{df::DataFrame}: A \texttt{DataFrame} containing the random phase angles to be transformed.

\end{itemize}
\textbf{Returns}

Nothing. The \texttt{DataFrame} is modified in place.

\textbf{Example}


\begin{minted}{julia}
w = 0:0.1:10
psd = CloughPenzien(w, 1.0, 2.0, 0.05, 3.0, 0.1)
t = 0:0.1:10
name = :process1
sr = SpectralRepresentation(psd, t, name)
samples = sample(sr, 5)
to_standard_normal_space!(sr, samples)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/stochasticprocesses/spectralrepresentation.jl#L131-L154}{\texttt{source}}


\end{adjustwidth}
\hypertarget{8606567393101569031}{\texttt{UncertaintyQuantification.to\_physical\_space!}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
to_physical_space!(sr::SpectralRepresentation, df::DataFrame) -> Nothing
\end{minted}

Transforms the random phase angles in the given \texttt{DataFrame} from a standard normal distribution to a uniform distribution.

\textbf{Arguments}

\begin{itemize}
\item \texttt{sr::SpectralRepresentation}: An instance of the \texttt{SpectralRepresentation} struct.


\item \texttt{df::DataFrame}: A \texttt{DataFrame} containing the random phase angles to be transformed.

\end{itemize}
\textbf{Returns}

Nothing. The \texttt{DataFrame} is modified in place.

\textbf{Example}


\begin{minted}{julia}
w = 0:0.1:10
psd = CloughPenzien(w, 1.0, 2.0, 0.05, 3.0, 0.1)
t = 0:0.1:10
name = :process1
sr = SpectralRepresentation(psd, t, name)
samples = sample(sr, 5)
to_standard_normal_space!(sr, samples)  # Transform to standard normal space
to_physical_space!(sr, samples)         # Transform back to physical space
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/stochasticprocesses/spectralrepresentation.jl#L162-L186}{\texttt{source}}


\end{adjustwidth}
\hypertarget{17794587236484596438}{\texttt{UncertaintyQuantification.dimensions}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
dimensions(sr::SpectralRepresentation) -> Int
\end{minted}

Returns the number of dimensions (frequencies) in the given \texttt{SpectralRepresentation} instance.

\textbf{Arguments}

\begin{itemize}
\item \texttt{sr::SpectralRepresentation}: An instance of the \texttt{SpectralRepresentation} struct.

\end{itemize}
\textbf{Returns}

An integer representing the number of dimensions (frequencies).

\textbf{Example}


\begin{minted}{julia}
w = 0:0.1:10
psd = CloughPenzien(w, 1.0, 2.0, 0.05, 3.0, 0.1)
t = 0:0.1:10
name = :process1
sr = SpectralRepresentation(psd, t, name)
num_dimensions = dimensions(sr)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/stochasticprocesses/spectralrepresentation.jl#L194-L215}{\texttt{source}}


\end{adjustwidth}
\hypertarget{9389867473198091497}{\texttt{Base.names}}  -- {Method.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
names(sr::SpectralRepresentation) -> Vector{Symbol}
\end{minted}

Returns the names of the random phase angles for a given \texttt{SpectralRepresentation} instance.

\textbf{Arguments}

\begin{itemize}
\item \texttt{sr::SpectralRepresentation}: An instance of the \texttt{SpectralRepresentation} struct.

\end{itemize}
\textbf{Returns}

A vector of symbols representing the names of the random phase angles.

\textbf{Example}


\begin{minted}{julia}
w = 0:0.1:10
psd = CloughPenzien(w, 1.0, 2.0, 0.05, 3.0, 0.1)
t = 0:0.1:10
name = :process1
sr = SpectralRepresentation(psd, t, name)
phase_angle_names = names(sr)
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/inputs/stochasticprocesses/spectralrepresentation.jl#L220-L241}{\texttt{source}}


\end{adjustwidth}

\chapter{SlurmInterface}


\section{SlurmInterface}



\label{260344389708059720}{}


\subsection{Index}



\label{6663683553518785561}{}

\begin{itemize}
\item \hyperlinkref{339239240194958289}{\texttt{UncertaintyQuantification.SlurmInterface}}
\end{itemize}


\subsection{Type}



\label{5754456517017253880}{}

\hypertarget{339239240194958289}{\texttt{UncertaintyQuantification.SlurmInterface}}  -- {Type.}

\begin{adjustwidth}{2em}{0pt}


\begin{minted}{julia}
SlurmInterface(options::Dict{String,String}, throttle::Integer, btachsize::Integer, extras::Vector{String})
\end{minted}

When \texttt{SlurmInterface} is passed to an \texttt{ExternalModel}, model evaluations are executed using slurm job arrays. This allows for heavier simulations or workflows to be sampled, without relying on Julia{\textquotesingle}s native parallelism. \texttt{SlurmInterface} automatically generates a slurm job array script, and Julia waits for this job to finish before extracting results.

When using \texttt{SlurmInterface}, you no longer need to load workers into Julia with \texttt{addprocs(N)}, and the requested nodes / tasks those required by individual model evaluations. Use \texttt{extras} to specify anything that must be preloaded for your models to be executed (for example loading modules).

The \texttt{throttle} specifies the number of simulations in the job array which are run concurrently. I.e., if you perform \texttt{MonteCarlo} simulation with \texttt{N=1000} samples, with \texttt{throttle=200}, it will run 1000 simulations in total, but only 200 at the same time. Your HPC scheduler (and admin) may be unhappy if you request too many concurrent jobs. If left empty, you scheduler{\textquotesingle}s default throttle will be used. In the case that your HPC machine limits the size of submitted job arrays, you can split the submissions into smaller {\textquotedbl}batches{\textquotedbl}. Specify {\textquotedbl}batchsize{\textquotedbl} to the maximum size of a job array. This does not change the total number of runs.

\textbf{parameters}


\begin{minted}{text}
options   : A dictionary of SBATCH options to add to the slurm script
throttle  : the number of jobs to be run at the same time
batchsize : maximum size of the slurm array, use when HPC limits the number of jobs in arrays
extras    : instructions to be executed before the model is run, e.g. activating a python environment or loading modules
\end{minted}

\textbf{Examples}


\begin{minted}{jlcon}
julia> slurm = SlurmInterface(Dict("account" => "HPC_account_1", "partition" => "CPU_partition"), extras = ["load python3"])
SlurmInterface(Dict("account" => "HPC_account_1", "partition" => "CPU_partition"), 0, 0, ["load python3"])
\end{minted}



\href{https://github.com/friesischscott/UncertaintyQuantification.jl/blob/f5ee6cce729f0d6a57979257379c942cdf42f86f/src/hpc/slurm.jl#L1-L24}{\texttt{source}}


\end{adjustwidth}

\part{References}


\chapter{References}



\label{14148394603223710272}{}


{\raggedright% @bibliography

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[1]}}\hypertarget{17677777724040662142}{}E.~Nikolaidis. \href{https://doi.org/10.1201/9780203483930}{\emph{Types of Uncertainty in Design Decision Making}}. In: \emph{Engineering Design Reliability Handbook}, edited by  (CRC Press, 2004).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[2]}}\hypertarget{5841225584812469631}{}J.~M.~Aughenbaugh and C.~J.~Paredis. \emph{The Value of Using Imprecise Probabilities in Engineering Design}. \href{https://doi.org/10.1115/1.2204976}{Journal~of~Mechanical~Design \textbf{128}, 969–979} (2005).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[3]}}\hypertarget{15087536451490649448}{}A.~D.~Kiureghian and O.~Ditlevsen. \emph{Aleatory or Epistemic? Does It Matter?} \href{https://doi.org/10.1016/j.strusafe.2008.06.020}{Structural~Safety \textbf{31}, 105–112} (2009).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[4]}}\hypertarget{3640249115689488589}{}S.~Bi, M.~Broggi, P.~Wei and M.~Beer. \emph{The Bhattacharyya Distance: Enriching the P-box in Stochastic Sensitivity Analysis}. \href{https://doi.org/10.1016/j.ymssp.2019.04.035}{Mechanical~Systems~and~Signal~Processing \textbf{129}, 265–281} (2019).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[5]}}\hypertarget{13630934291971494037}{}S.~Ferson, V.~Kreinovick, L.~Ginzburg, F.~Sentz and D.~Meyers. \href{https://doi.org/10.2172/809606}{\emph{Constructing Probability Boxes and Dempster-Shafer Structures}}. Technical Report~SAND2015-4166J (Sandia National Laboratory, 2015).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[6]}}\hypertarget{4623761477557390374}{}M.~Besançon, T.~Papamarkou, D.~Anthoff, A.~Arslan, S.~Byrne, D.~Lin and J.~Pearson. \emph{Distributions.jl: Definition and Modeling of Probability Distributions in the JuliaStats Ecosystem}. \href{https://doi.org/10.18637/jss.v098.i16}{Journal~of~Statistical~Software \textbf{98}, 1–30} (2021).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[7]}}\hypertarget{15494272573324799614}{}M.~Sklar. \emph{Fonctions de repartition a n dimensions et leurs marges}. Publ.~inst.~statist.~univ.~Paris \textbf{8}, 229–231 (1959).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[8]}}\hypertarget{4026133216555554139}{}H.~Joe. \emph{Dependence Modeling with Copulas} (CRC Press, 2015).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[9]}}\hypertarget{3820492032986589588}{}B.~W.~Silverman. \emph{Density Estimation for Statistics and Data Analysis}. \emph{Monographs on Statistics and Applied Probability} (Chapman \& Hall, 1986).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[10]}}\hypertarget{15023702097247238757}{}S.~J.~Sheather and M.~C.~Jones. \emph{A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation}. \href{https://doi.org/10.1111/j.2517-6161.1991.tb01857.x}{Journal~of~the~Royal~Statistical~Society:~Series~B~(Methodological) \textbf{53}, 683–690} (1991).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[11]}}\hypertarget{5290438035122008794}{}I.~Papaioannou and D.~Straub. \emph{Combination Line Sampling for Structural Reliability Analysis}. \href{https://doi.org/10.1016/j.strusafe.2020.102025}{Structural~Safety \textbf{88}, 102025} (2021).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[12]}}\hypertarget{9935717079447158324}{}R.~Rackwitz and B.~Flessler. \emph{Structural Reliability under Combined Random Load Sequences}. \href{https://doi.org/10.1016/0045-7949(78)90046-9}{Computers~\&~Structures \textbf{9}, 489–494} (1978).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[13]}}\hypertarget{7384485740282643180}{}R.~Melchers. \emph{Importance Sampling in Structural Systems}. \href{https://doi.org/10.1016/0167-4730(89)90003-9}{Structural~Safety \textbf{6}, 3–10} (1989).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[14]}}\hypertarget{8183389152391755847}{}A.~Harbitz. \emph{An Efficient Sampling Method for Probability of Failure Calculation}. \href{https://doi.org/10.1016/0167-4730(86)90012-3}{Structural~Safety \textbf{3}, 109–115} (1986).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[15]}}\hypertarget{9920150670139184364}{}P.~Koutsourelakis, H.~Pradlwarter and G.~Schuëller. \emph{Reliability of Structures in High Dimensions, Part I: Algorithms and Applications}. \href{https://doi.org/10.1016/j.probengmech.2004.05.001}{Probabilistic~Engineering~Mechanics \textbf{19}, 409–417} (2004).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[16]}}\hypertarget{12068672950609541734}{}M.~De Angelis, E.~Patelli and M.~Beer. \emph{Advanced Line Sampling for Efficient Robust Reliability Analysis}. \href{https://doi.org/10.1016/j.strusafe.2014.10.002}{Structural~Safety \textbf{52}, 170–182} (2015).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[17]}}\hypertarget{15932307010839829307}{}S.-K.~Au and J.~L.~Beck. \emph{Estimation of Small Failure Probabilities in High Dimensions by Subset Simulation}. \href{https://doi.org/10.1016/S0266-8920(01)00019-4}{Probabilistic~Engineering~Mechanics \textbf{16}, 263–277} (2001).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[18]}}\hypertarget{345953961655055560}{}S.-K.~Au and E.~Patelli. \emph{Rare Event Simulation in Finite-Infinite Dimensional Space}. \href{https://doi.org/10.1016/j.ress.2015.11.012}{Reliability~Engineering~\&~System~Safety \textbf{148}, 67–77} (2016).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[19]}}\hypertarget{3929737418388848918}{}A.~B.~Owen. \emph{Monte Carlo and Quasi-Monte Carlo for Statistics}. In: \emph{Monte Carlo and Quasi-Monte Carlo Methods 2008} (2009); pp.~3–18.

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[20]}}\hypertarget{8155427533001242046}{}A.~B.~Owen. \href{https://artowen.su.domains/reports/rhalton.pdf}{\emph{A randomized Halton Algorithm in R}} (2017).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[21]}}\hypertarget{7743418060059618696}{}N.~Metropolis, A.~W.~Rosenbluth, M.~N.~Rosenbluth, A.~H.~Teller and E.~Teller. \emph{Equation of State Calculations by Fast Computing Machines}. \href{https://doi.org/10.1063/1.1699114}{The~Journal~of~Chemical~Physics \textbf{21}, 1087–1092} (1953).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[22]}}\hypertarget{1755040324625540940}{}W.~K.~Hastings. \emph{Monte Carlo Sampling Methods Using Markov Chains and Their Applications}. \href{https://doi.org/10.1093/biomet/57.1.97}{Biometrika \textbf{57}, 97–109} (1970).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[23]}}\hypertarget{16839907763403924006}{}H.~Raiffa and R.~Schaifer. \emph{Applied Statistical Decision Theory}. \emph{Studies in Managerial Economics} (Division of Research, Graduate School of Business Adminitration, Harvard University, 1961).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[24]}}\hypertarget{7619550157776039859}{}J.~Ching and Y.-C.~Chen. \emph{Transitional Markov Chain Monte Carlo Method for Bayesian Model Updating, Model Class Selection, and Model Averaging}. \href{https://doi.org/10.1061/(ASCE)0733-9399(2007)133:7(816)}{Journal~of~Engineering~Mechanics \textbf{133}, 816–832} (2007).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[25]}}\hypertarget{2227099348983167607}{}J.~Li and J.~Chen. \href{https://www.wiley.com/en-us/Stochastic+Dynamics+of+Structures-p-9780470824252}{\emph{Stochastic Dynamics of Structures}}. 1~Edition (John Wiley \& Sons, Ltd (Asia), United States, 2009).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[26]}}\hypertarget{13534374753547515349}{}K.~Kanai. \emph{Semi-empirical formula for the seismic characteristics of the ground}. Earthquake~Research~Institute \textbf{35}, 309–325 (1957).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[27]}}\hypertarget{5778133231777751075}{}H.~Tajimi. \emph{A statistical method of determining the maximum response of a building structure during an earthquake}. In: \emph{Proceedings of the 2nd World Conference on Earthquake Engineering, Tokyo, Japan, 1960} (1960); pp.~781–797.

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[28]}}\hypertarget{16593283455700884559}{}R.~Clough and J.~Penzien. \emph{Dynamics of Structures} (McGraw-Hill, 1975).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[29]}}\hypertarget{17514431937910934166}{}M.~Shinozuka and G.~Deodatis. \emph{Simulation of stochastic processes by spectral representation}. \href{https://doi.org/10.1115/1.3119501}{Applied~Mechanics~Reviews \textbf{44}, 191–204} (1991).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[30]}}\hypertarget{17411345754570781475}{}E.~Patelli and S.~K.~Au. \href{https://www.researchgate.net/publication/280224186\_Efficient\_Monte\_Carlo\_Algorithm\_For\_Rare\_Failure\_Event\_Simulation}{\emph{Efficient Monte Carlo Algorithm for Rare Failure Event Simulation}}. In: \emph{12th International Conference on Applications of Statistics and Probability in Civil Engineering, ICASP 2012} (University of British Columbia, Jul 2015).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[31]}}\hypertarget{8907925646719725278}{}I.~Papaioannou, W.~Betz, K.~Zwirglmaier and D.~Straub. \emph{MCMC algorithms for subset simulation}. Probabilistic~Engineering~Mechanics \textbf{41}, 89–103 (2015).

\hangindent=0.33in {\makebox[{\ifdim0.33in<\dimexpr\width+1ex\relax\dimexpr\width+1ex\relax\else0.33in\fi}][l]{[32]}}\hypertarget{2393110712994691496}{}J.~Chan, I.~Papaioannou and D.~Straub. \emph{An adaptive subset simulation algorithm for system reliability analysis with discontinuous limit states}. Reliability~Engineering~\&~System~Safety \textbf{225}, 108607 (2022).

}% end @bibliography


\end{document}
